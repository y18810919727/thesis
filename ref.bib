% !Mode:: "TeX:UTF-8"

@book{oetiker2002,
   author = {Oetiker, Tobias and 中国CTEX用户小组},
   title = {一份不太简短的LATEX介绍},
   year = {2002},
   type = {Book}
}
@article{chenweide2009,
   author = {陈德伟},
   title = {用LaTeX撰写学位论文},
   journal = {软件导刊},
   number = {11},
   pages = {100-102},
   abstract = {学位论文作为高校教学计划中的重要环节,对提高教学质量、培养学生综合应用能力具有十分重要的意义。LaTeX是一种格式化的排版系统,将它应用于学位论文的撰写,真正做到编辑、排版、校对一体化,在较高层次上实现学位论文排版的美感。讨论了如何采用LaTeX系统来撰写学位论文。如果将此作为学位论文模板共享将大大节省学位论文的排版时间,提高高校学位论文规范化管理的质量。},
   keywords = {LaTeX系统
学位论文
科技排版},
   ISSN = {1672-7800},
   year = {2009},
   type = {Journal Article}
}

@article{duanmaiying2003,
   author = {段麦英},
   title = {英文科技论文的LaTeX排版},
   journal = {雁北师范学院学报},
   number = {02},
   pages = {22-24},
   abstract = {LaTeX是普遍采用的通用英文科技出版物排版软件之一 ,国际英文物理学期刊无一例外都接受LaTeX格式的文稿 .在本文中 ,笔者结合自己使用LaTeX排版的一些经验 ,介绍了LaTeX排版软件基本命令的使用方法和技巧 .},
   keywords = {英文科技论文
LaTeX排版
方法和技巧},
   ISSN = {1009-1939},
   year = {2003},
   type = {Journal Article}
}

@article{jihongwei2011,
   author = {纪宏伟},
   title = {数学论文的LaTeX排版与全文上网},
   journal = {软件导刊(教育技术)},
   number = {01},
   pages = {87-88},
   abstract = {LaTeX是一种格式化的排版系统,将它应用于数学论文的排版,可在较高层次上实现数学论文排版的美感。介绍了LaTeX的排版过程及其排版功能,探讨了数学论文全文上网的实现手段,并提供了切实可行的简便方法。},
   keywords = {LaTeX
排版系统
PDF文档
数据库},
   ISSN = {1672-7800},
   year = {2011},
   type = {Journal Article}
}

@article{majiajia2014,
   author = {马加佳},
   title = {LaTeX与Word文件的相互转换},
   journal = {中国科技期刊研究},
   number = {03},
   pages = {378-382},
   abstract = {利用TeX2Word与Word2TeX软件,可以迅速实现Word与LaTeX文件的互相转换,使排版人员或作者在重新排版时减少工作量,提高排版效率。分别介绍这两款软件的使用方法,通过转换前后的效果对比,探讨Word与LaTeX文件互转的可行性及需要注意的问题。},
   keywords = {排版
LaTeX Word转换
TeX2Word Word2TeX},
   ISSN = {1001-7143},
   year = {2014},
   type = {Journal Article}
}

@article{niejun2010,
   author = {聂俊 and 陈天莹 and 符红光},
   title = {基于Latex的互联网数学公式搜索引擎},
   journal = {计算机应用},
   number = {S2},
   pages = {312-315},
   abstract = {互联网上数学公式的搜索对学习和科研非常重要,目前Google、百度等著名搜索引擎还没提供类似的服务。除图片外,数学公式在互联网上主要以Latex和MathML的形式存在。对这两种数学公式格式,至今仍未出现成熟的数学公式搜索引擎。以Latex格式为基础(MathML可转为Latex格式),提出数学公式的分词、索引和匹配算法,并在开源的Lucene平台上实现了一个高效的数学公式搜索引擎。},
   keywords = {数学公式
搜索引擎
Latex
MathML},
   ISSN = {1001-9081},
   year = {2010},
   type = {Journal Article}
}

@article{wangyong2012,
   author = {王勇 and 姚萍 and 王岚 and 庞立},
   title = {LaTeX与方正书版排版数学论文探讨},
   journal = {中国科技期刊研究},
   number = {06},
   pages = {1036-1039},
   abstract = {从获取渠道、排版程序、排版效果、排版命令以及最终文稿等方面分析比较了LaTeX和方正书版2种软件排版数学论文的特点。比较发现,在排版数学论文方面,LaTeX软件更有优势。},
   keywords = {LaTeX
方正书版
数学论文},
   ISSN = {1001-7143},
   year = {2012},
   type = {Journal Article}
}

@article{wenyayuan2012,
   author = {温亚媛 and 赵景芝 and 李向华 and 谢冰蓉},
   title = {LaTeX排版系统在英文学术期刊中的应用},
   journal = {中国科技期刊研究},
   number = {05},
   pages = {825-830},
   abstract = {结合英文学术期刊使用LaTeX排版系统的工作经验,总结出一些使用该系统排版的心得体会。分析整理作者来稿中经常出现的问题,给出了解决的办法。同时给出我们在日常编辑工作中常用的宏包和模板,以及一些排版小技巧,与编辑同行交流。希望能对LaTeX初学者及正在使用LaTeX撰写论文的年轻作者有所帮助。},
   keywords = {LaTeX
排版
技巧
方法},
   ISSN = {1001-7143},
   year = {2012},
   type = {Journal Article}
}
@inproceedings{lutter2019deep,
  title={Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning},
  author={Lutter, M and Ritter, C and Peters, Jan},
  booktitle={International Conference on Learning Representations (ICLR 2019)},
  year={2019},
  organization={OpenReview. net}
}

@article{gupta2019general,
  title={A general framework for structured learning of mechanical systems},
  author={Gupta, Jayesh K and Menda, Kunal and Manchester, Zachary and Kochenderfer, Mykel J},
  journal={arXiv preprint arXiv:1902.08705},
  year={2019}
}

@inproceedings{sanchez2018graph,
  title={Graph networks as learnable physics engines for inference and control},
  author={Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter},
  booktitle={International Conference on Machine Learning},
  pages={4470--4479},
  year={2018},
  organization={PMLR}
}

@article{greydanus2019hamiltonian,
  title={Hamiltonian Neural Networks},
  author={Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={15379--15389},
  year={2019}
}

@book{box2015time,
  title={Time series analysis: forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}

@article{benjamin2003generalized,
  title={Generalized autoregressive moving average models},
  author={Benjamin, Michael A and Rigby, Robert A and Stasinopoulos, D Mikis},
  journal={Journal of the American Statistical association},
  volume={98},
  number={461},
  pages={214--223},
  year={2003},
  publisher={Taylor \& Francis}
}

@article{zhang2021DLgeneral,
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {3},
issn = {0001-0782},
journal = {Commun. ACM},
month = feb,
pages = {107–115},
numpages = {9}
}

@article{karpatne2017theory,
  title={Theory-guided data science: A new paradigm for scientific discovery from data},
  author={Karpatne, Anuj and Atluri, Gowtham and Faghmous, James H and Steinbach, Michael and Banerjee, Arindam and Ganguly, Auroop and Shekhar, Shashi and Samatova, Nagiza and Kumar, Vipin},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={29},
  number={10},
  pages={2318--2331},
  year={2017},
  publisher={IEEE}
}

@inproceedings{doan2019physics,
  title={Physics-informed echo state networks for chaotic systems forecasting},
  author={Doan, Nguyen Anh Khoa and Polifke, Wolfgang and Magri, Luca},
  booktitle={International Conference on Computational Science},
  pages={192--198},
  year={2019},
  organization={Springer}
}

@inproceedings{baseman2018physics,
  title={Physics-informed machine learning for DRAM error modeling},
  author={Baseman, Elisabeth and DeBardeleben, Nathan and Blanchard, Sean and Moore, Juston and Tkachenko, Olena and Ferreira, Kurt and Siddiqua, Taniya and Sridharan, Vilas},
  booktitle={2018 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}

@article{park2019physics,
  title={Physics-induced graph neural network: An application to wind-farm power estimation},
  author={Park, Junyoung and Park, Jinkyoo},
  journal={Energy},
  volume={187},
  pages={115883},
  year={2019},
  publisher={Elsevier}
}

@article{lagergren2020learning,
  title={Learning partial differential equations for biological transport models from noisy spatio-temporal data},
  author={Lagergren, John H and Nardini, John T and Michael Lavigne, G and Rutter, Erica M and Flores, Kevin B},
  journal={Proceedings of the Royal Society A},
  volume={476},
  number={2234},
  pages={20190800},
  year={2020},
  publisher={The Royal Society Publishing}
}

@article{schmidt2009distilling,
  title={Distilling free-form natural laws from experimental data},
  author={Schmidt, Michael and Lipson, Hod},
  journal={science},
  volume={324},
  number={5923},
  pages={81--85},
  year={2009},
  publisher={American Association for the Advancement of Science}
}

@article{yang2018physics,
  title={Physics-informed deep generative models},
  author={Yang, Yibo and Perdikaris, Paris},
  journal={arXiv preprint arXiv:1812.03511},
  year={2018}
}

@article{raissi2018deep,
  title={Deep hidden physics models: Deep learning of nonlinear partial differential equations},
  author={Raissi, Maziar},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={932--955},
  year={2018},
  publisher={JMLR. org}
}

@article{ZHU201956,
title = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
journal = {Journal of Computational Physics},
volume = {394},
pages = {56-81},
year = {2019},
issn = {0021-9991},
author = {Yinhao Zhu and Nicholas Zabaras and Phaedon-Stelios Koutsourelakis and Paris Perdikaris},
}

@article{RAISSI2019686,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
}

@article{weinan2017proposal,
  title={A proposal on machine learning via dynamical systems},
  author={Weinan, E},
  journal={Communications in Mathematics and Statistics},
  volume={5},
  number={1},
  pages={1--11},
  year={2017},
  publisher={Springer}
}

@inproceedings{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={6572--6583},
  year={2018}
}

@article{kidger2020neural,
  title={Neural controlled differential equations for irregular time series},
  author={Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},
  journal={arXiv preprint arXiv:2005.08926},
  year={2020}
}

@inproceedings{brouwer2019gru,
  title={GRU-ODE-Bayes: continuous modeling of sporadically-observed time series},
  author={Brouwer, Edward De and Simm, Jaak and Arany, Adam and Moreau, Yves},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={7379--7390},
  year={2019}
}

@article{jordan2021gated,
  title={Gated recurrent units viewed through the lens of continuous time dynamical systems},
  author={Jordan, Ian D and Sok{\'o}{\l}, Piotr Aleksander and Park, Il Memming},
  journal={Frontiers in computational neuroscience},
  pages={67},
  year={2021},
  publisher={Frontiers}
}
@article{zhang2019anodev2,
  title={ANODEV2: A coupled neural ODE framework},
  author={Zhang, Tianjun and Yao, Zhewei and Gholami, Amir and Gonzalez, Joseph E and Keutzer, Kurt and Mahoney, Michael W and Biros, George},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={5151--5161},
  year={2019}
}


@INPROCEEDINGS{SeducePastor2018, 
author={J. {Pastor} and J. M. {Menaud}}, 
booktitle={Proceedings of the 26th International Conference on Software, Telecommunications and Computer Networks (SoftCOM2018)}, 
title={SeDuCe: a Testbed for Research on Thermal and Power Management in Datacenters}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-6}, 
ISSN={1847-358X}, 
month={Sep.},
}

@incollection{grid5000,
   title = {Adding Virtualization Capabilities to the {Grid'5000} Testbed},
   author = {Balouek, Daniel and Carpen Amarie, Alexandra and Charrier, Ghislain and Desprez, Fr{\'e}d{\'e}ric and Jeannot, Emmanuel and Jeanvoine, Emmanuel and L{\`e}bre, Adrien and Margery, David and Niclausse, Nicolas and Nussbaum, Lucas and Richard, Olivier and P{\'e}rez, Christian and Quesnel, Flavien and Rohr, Cyril and Sarzyniec, Luc},
   booktitle = {Cloud Computing and Services Science},
   publisher = {Springer International Publishing},
   pages = {3-20},
   volume = {367},
   editor = {Ivanov, Ivan I. and van Sinderen, Marten and Leymann, Frank and Shan, Tony },
   series = {Communications in Computer and Information Science },
   isbn = {978-3-319-04518-4 },
   year = {2013},
}

@misc{InrowACRD602,
  title={InRow Direct Expansion (ACRD602)},
  author={{Schneider Electric}},
  year={2019},
  note={accessed: December 15,2019}
}

@article{alonso2020estimating,
  title={Estimating cooling production and monitoring efficiency in chillers using a soft sensor},
  author={Alonso, Seraf{\'\i}n and Mor{\'a}n, Antonio and P{\'e}rez, Daniel and Prada, Miguel A and Diaz, Ignacio and Dom{\'\i}nguez, Manuel},
  journal={Neural Computing and Applications},
  volume={32},
  number={23},
  pages={17291--17308},
  year={2020},
  publisher={Springer}
}

@techreport{ashraeTC992011,
  title={2011 Thermal Guidelines for Data Processing Environments – Expanded Data Center Classes and Usage Guidance},
  author={ASHRAE Technical Committee},
  year={2011},
  institution={ASHRAE},
}

@article{hopcroft2001introduction,
  title={Introduction to automata theory, languages, and computation},
  author={Hopcroft, John E and Motwani, Rajeev and Ullman, Jeffrey D},
  journal={Acm Sigact News},
  volume={32},
  number={1},
  pages={60--65},
  year={2001},
  publisher={ACM New York, NY, USA}
}

@article{Deep_state_space_model,
abstract = {We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and inter-pretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from large collection of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.},
author = {Deep state space models for time series forecastingRangapuram, Syama Sundar and Seeger, Matthias and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Paper 4852$\backslash$$\backslash$deep-state-long-version.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {7785--7794},
title = {{Deep state space models for time series forecasting}},
volume = {2018-Decem},
year = {2018}
}

@article{Fraccaro2017,
abstract = {This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.},
archivePrefix = {arXiv},
arxivId = {1710.05741},
author = {Fraccaro, Marco and Kamronn, Simon and Paquet, Ulrich and Winther, Ole},
eprint = {1710.05741},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {VAE},
number = {section 5},
pages = {3602--3611},
title = {{A disentangled recognition and nonlinear dynamics model for unsupervised learning}},
volume = {2017-Decem},
year = {2017}
}

@article{Knauf2018,
abstract = {Differential equations are as varied as the phenomena of nature described by them.},
archivePrefix = {arXiv},
arxivId = {1806.07366},
author = {Knauf, Andreas},
eprint = {1806.07366},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Knauf - 2018 - Neural Ordinary Differential Equations.pdf:pdf},
issn = {20385757},
journal = {UNITEXT - La Matematica per il 3 piu 2},
mendeley-groups = {machine learning},
number = {NeurIPS},
pages = {31--60},
title = {{Neural Ordinary Differential Equations}},
volume = {109},
year = {2018}
}

@article{nason2006stationary,
  title={Stationary and non-stationary time series},
  author={Nason, Guy P},
  journal={Statistics in volcanology},
  volume={60},
  year={2006},
  publisher={Geological Society of London, London}
}
@article{Demeester2019,
archivePrefix = {arXiv},
arxivId = {1911.09431},
author = {Demeester, Thomas},
eprint = {1911.09431},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Demeester - 2019 - System Identification with Time-Aware Neural Sequence Models.pdf:pdf},
mendeley-groups = {system_identify},
title = {{System Identification with Time-Aware Neural Sequence Models}},
year = {2019}
}
@inbook{10.5555/3454287.3454765,
author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David},
title = {Latent ODEs for Irregularly-Sampled Time Series},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Time series with non-uniform intervals occur in many applications, and are difficult
to model using standard recurrent neural networks (RNNs). We generalize RNNs to have
continuous-time hidden dynamics defined by ordinary differential equations (ODEs),
a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition
network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can
naturally handle arbitrary time gaps between observations, and can explicitly model
the probability of observation times using Poisson processes. We show experimentally
that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled
data.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {478},
numpages = {11}
}
@article{Hafner2019,
abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
archivePrefix = {arXiv},
arxivId = {1811.04551},
author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
eprint = {1811.04551},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Hafner et al. - 2019 - Learning latent dynamics for planning from pixels(2).pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
mendeley-groups = {VAE},
pages = {4528--4547},
title = {{Learning latent dynamics for planning from pixels}},
volume = {2019-June},
year = {2019}
}
@article{yuan2020dual,
  title={A dual-attention recurrent neural network method for deep cone thickener underflow concentration prediction},
  author={Yuan, Zhaolin and Hu, Jinlong and Wu, Di and Ban, Xiaojuan},
  journal={Sensors},
  volume={20},
  number={5},
  pages={1260},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{du2020multivariate,
  title={Multivariate time series forecasting via attention-based encoder--decoder framework},
  author={Du, Shengdong and Li, Tianrui and Yang, Yan and Horng, Shi-Jinn},
  journal={Neurocomputing},
  volume={388},
  pages={269--279},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{li2016scalable,
  title={A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification},
  author={Li, Steven Cheng-Xian and Marlin, Benjamin},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages={1812--1820},
  year={2016}
}
@inproceedings{shukla2018interpolation,
  title={Interpolation-Prediction Networks for Irregularly Sampled Time Series},
  author={Shukla, Satya Narayan and Marlin, Benjamin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{Yildiz2019,
archivePrefix = {arXiv},
arxivId = {1905.10994},
author = {Yildiz, {\c{C}}agatay and Heinonen, Markus and L{\"{a}}hdesm{\"{a}}ki, Harri},
eprint = {1905.10994},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1905.10994_副本.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {machine learning,VAE},
number = {NeurIPS},
title = {{ODE2VAE: Deep generative second order ODEs with Bayesian neural networks}},
volume = {32},
year = {2019}
}


@article{WANG2013,
abstract = {The traditional paradigm of system identification employs prior information on system structures and environments and input/output observation data to derive system models. Extensive research and development on its methodologies, theoretical foundation, algorithms, verifications, and applications over the past half century have established a mature field with a rich literature and substantial benchmark applications. However, rapid advancement in science, technology, engineering, and social medias has ushered in a new era of systems science and control in which challenges and opportunities are abundant for system identification. In this sense, system identification remains an exciting, young, viable, and critical field that mandates new paradigms to meet such challenges. This article points out some potentially important aspects of system identification in these new paradigms, suggests some worthy areas of research focus, and most importantly opens the forum for further discussions.},
author = {WANG, Le-Yi and ZHAO, Wen-Xiao},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/wang2013.pdf:pdf},
issn = {18741029},
journal = {Acta Automatica Sinica},
keywords = {2013,39,7,933,942,System identification,acta automatica,and opportunities,challenges,citation wang le-yi,complexity,identification and decision,information,integration of,integration of identification and decision,large data processing,networked system,new paradigms,sinica,system identification,uncertainty,zhao wen-xiao},
mendeley-groups = {system{\_}identify},
number = {7},
pages = {933--942},
publisher = {The Chinese Association of Automation and The Institute of Automation, Chinese Academy of Sciences},
title = {{System Identification: New Paradigms, Challenges, and Opportunities}},
volume = {39},
year = {2013}
}

@ARTICLE{9260162,
  author={Liu, Haoyue and Chatterjee, Ishani and Zhou, MengChu and Lu, Xiaoyu Sean and Abusorrah, Abdullah},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Aspect-Based Sentiment Analysis: A Survey of Deep Learning Methods}, 
  year={2020},
  volume={7},
  number={6},
  pages={1358-1375},
  }
  
@ARTICLE{9161367,
  author={Liu, Jin and Wu, NaiQi and Qiao, Yan and Li, ZhiWu},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Short-Term Traffic Flow Forecasting Using Ensemble Approach Based on Deep Belief Networks}, 
  year={2020},
  volume={},
  number={},
  pages={1-14}
  }
  
@ARTICLE{9326384,
  author={Li, Huifang and Hu, Guangzheng and Li, Jianqiang and Zhou, Mengchu},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Intelligent Fault Diagnosis for Large-Scale Rotating Machines Using Binarized Deep Neural Networks and Random Forests}, 
  year={2021},
  volume={},
  number={},
  pages={1-11}
  }

@article{Essien2020,
author = {Essien, Aniekan Emmanuel and Giannetti, Cinzia},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/A{\_}Deep{\_}Learning{\_}model{\_}for{\_}Smart{\_}Manufacturing{\_}usin.pdf:pdf},
issn = {1551-3203},
journal = {IEEE Transactions on Industrial Informatics},
mendeley-groups = {time series},
volume={16},
number={9},
pages={6069--6078},
title = {{A Deep Learning model for Smart Manufacturing using Convolutional LSTM Neural Network Autoencoders}},
year = {2020}
}
@ARTICLE{4019326,
  author={M. {GEVERS}},
  journal={IEEE Control Systems Magazine}, 
  title={A personal view of the development of system identification: A 30-year journey through an exciting field}, 
  year={2006},
  volume={26},
  number={6},
  pages={93-105},
}
@article{Wang1998,
abstract = {This paper proposes the Runge-Kutta neural networks (RKNN's) for identification of unknown dynamical systems described by ordinary differential equations (i.e., ordinary differential equation or ODE systems) in high accuracy. These networks are constructed according to the Runge-Kutta approximation method. The main attraction of the RKNN's is that they precisely estimate the changing rates of system states (i.e., the right-hand side of the ODE x = f(x)) directly in their subnetworks based on the space-domain interpolation within one sampling interval such that they can do long-term prediction of system state trajectories. We show theoretically the superior generalization and long-term prediction capability of the RKNN's over the normal neural networks. Two types of learning algorithms are investigated for the RKNN's, gradient-and nonlinear recursive least-squares-based algorithms. Convergence analysis of the learning algorithms is done theoretically. Computer simulations demonstrate the proved properties of the RKNN's. {\textcopyright} 1998 IEEE.},
author = {Wang, Yi Jen and Lin, Chin Teng},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/yi-jenwang1998.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Contraction mapping,Gradient descent,Nonlinear recursive least square,Radial-basis function,Runge-Kutta method,Vander Pol's equation},
mendeley-groups = {system{\_}identify},
number = {2},
pages = {294--307},
title = {{Runge-Kutta neural network for identification of dynamical systems in high accuracy}},
volume = {9},
year = {1998}
}
@article{Weiss2017Sequence,
  title={Sequence-to-Sequence Models Can Directly Translate Foreign Speech},
  author={Weiss, Ron J and Chorowski, Jan and Jaitly, Navdeep and Wu, Yonghui and Chen, Zhifeng},
  journal={Proc. Interspeech 2017},
  pages={2625--2629},
  year={2017}
}
@article{Cho2014Learning,
  title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and Van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={Computer Science},
  year={2014},
}
@inproceedings{Demeester2020SystemIW,
  title={System identification with time-aware neural sequence models},
  author={Demeester, Thomas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3757--3764},
  year={2020}
}
@article{CHAI201661,
  title={An intelligent switching control for a mixed separation thickener process},
  author={Chai, Tianyou and Jia, Yao and Li, Haibo and Wang, Hong},
  journal={Control Engineering Practice},
  volume={57},
  pages={61--71},
  year={2016},
  publisher={Elsevier}
}
@article{KIM2004403,
  title={Development and application of a dynamic model for hindered-settling column separations},
  author={Kim, BH and Klima, Mark Stephen},
  journal={Minerals engineering},
  volume={17},
  number={3},
  pages={403--410},
  year={2004},
  publisher={Elsevier}
}
@incollection{NIPS2018_7892,
title = {Neural Ordinary Differential Equations},
author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {6571--6583},
year = {2018},
publisher = {Curran Associates, Inc.},
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{Botchkarev2019,
abstract = {Aim/Purpose The aim of this study was to analyze various performance metrics and approaches to their classification. The main goal of the study was to develop a new typology that will help to advance knowledge of metrics and facilitate their use in machine learning regression algorithms Background Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. A performance metric can be defined as a logical and mathematical construct designed to measure how close are the actual results from what has been expected or predicted. A vast variety of performance metrics have been described in academic literature. The most commonly mentioned metrics in research studies are Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), etc. Knowledge about metrics properties needs to be systematized to simplify the design and use of the metrics. Methodology A qualitative study was conducted to achieve the objectives of identifying related peer-reviewed research studies, literature reviews, critical thinking and inductive reasoning. Contribution The main contribution of this paper is in ordering knowledge of performance metrics and enhancing understanding of their structure and properties by proposing a new typology, generic primary metrics mathematical formula and a visualization chart Findings Based on the analysis of the structure of numerous performance metrics, we proposed a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set. For each component, implementation options have been identified. The suggested new typology has been shown to cover a total of over 40 commonly used primary metrics Recommendations Presented findings can be used to facilitate teaching performance metrics to for Practitioners university students and expedite metrics selection and implementation processes for practitioners Recommendations By using the proposed typology, researchers can streamline development of for Researchers new metrics with predetermined properties Impact on Society The outcomes of this study could be used for improving evaluation results in machine learning regression, forecasting and prognostics with direct or indirect positive impacts on innovation and productivity in a societal sense Future Research Future research is needed to examine the properties of the extended metrics, composite metrics, and hybrid sets of metrics. Empirical study of the metrics is needed using R Studio or Azure Machine Learning Studio, to find associations between the properties of primary metrics and their “numerical” behavior in a wide spectrum of data characteristics and business or research requirements.},
author = {Botchkarev, Alexei},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/IJIKMv14p045-076Botchkarev5064.pdf:pdf},
issn = {15551237},
journal = {Interdisciplinary Journal of Information, Knowledge, and Management},
keywords = {Accuracy measures,Classification,Dissimilarity,Distance,Error measures,Estimation,Evaluation,Forecasting,Machine learning,Modeling,Performance metrics,Prediction,Prognostics,Properties,Regression,Similarity,Typology},
mendeley-groups = {Regression metrics},
number = {January},
pages = {45--76},
title = {{A new typology design of performance metrics to measure errors in machine learning regression algorithms}},
volume = {14},
year = {2019}
}
@article{hongjiang2011study,
  title={Study on the thickening properties of unclassified tailings and its application to thickener design},
  author={Hongjiang, Wang and Qinrui, Chen and Aixiang, Wu and others},
  journal={Journal of University of Science and Technology Beijing},
  volume={6},
  pages={676--681},
  year={2011}
}

@inproceedings{Kratochwil1997A,
  title={A simple algorithm for asymptotically optimal reduced-state sequence estimation},
  author={Kratochwil, Konrad},
  booktitle={Proceedings of ICUPC 97-6th International Conference on Universal Personal Communications},
  volume={2},
  pages={718--722},
  year={1997},
  organization={IEEE}
}
@inproceedings{bahdanau2014neural,
author = {Dzmitry Bahdanau and
Kyunghyun Cho and
Yoshua Bengio},
bibsource = {dblp computer science bibliography, https://dblp.org},
booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
 title = {Neural Machine Translation by Jointly Learning to Align and Translate},
 year = {2015}
}
@inproceedings{Merri2014,
 address = {Doha, Qatar},
 author = {Cho, Kyunghyun  and
van Merri{\"e}nboer, Bart  and
Bahdanau, Dzmitry  and
Bengio, Yoshua},
 booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
 pages = {103--111},
 publisher = {Association for Computational Linguistics},
 title = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
 year = {2014}
}

@article{Zhang2018_02,
  title={Residual dense network for image restoration},
  author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}
@article{Huang2017,
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
eprint = {1608.06993},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1608.06993.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
mendeley-groups = {machine learning},
pages = {4700--4708},
title = {{Densely connected convolutional networks}},
volume = {2017-January},
year = {2017}
}


@inproceedings{contextaware,
author = {Zhou, Yang and Huang, Yan},
year = {2018},
month = {12},
pages = {2393-2402},
title = {Context Aware Flow Prediction of Bike Sharing Systems},
}
@article{HuangAn2014,
  title={An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels},
  author={Huang and Guang-Bin},
  journal={Cognitive Computation 2014},
  volume={6},
  number={3},
  pages={376-390},
}
@article{Ke2017,
abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie Yan},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {machine learning},
number = {Nips},
pages = {3146--3154},
title = {{LightGBM: A highly efficient gradient boosting decision tree}},
volume = {2017-December},
year = {2017}
}
@inproceedings{Wu:2019:CAC:3357384.3358133,
 author = {Wu, Di and Wang, Hao and Seidu, Razak},
 title = {Collaborative Analysis for Computational Risk in Urban Water Supply Systems},
 booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
 series = {CIKM '19},
 year = {2019},
 isbn = {978-1-4503-6976-3},
 location = {Beijing, China},
 pages = {2297--2300},
 numpages = {4},
 acmid = {3358133},
 keywords = {collaborative analysis, computational risk, structural similarity, urban water supply, water quality control},
} 
@ARTICLE{wu2019_2,
author={D. {Wu} and H. {Wang} and H. {Mohammed} and R. {Seidu}},
journal={IEEE Transactions on Sustainable Computing},
title={Quality Risk Analysis for Sustainable Smart Water Supply Using Data Perception},
year={2019},
volume={},
number={},
pages={1-1},
keywords={Sustainable Water Supply;Water Quality Control;Data Perception;Risk Evaluation;Frequency Analysis;Scalability},
ISSN={2377-3790},
month={},}

@article{Zhou:2019:LCN:3368406.3339308,
  title={Lightweight convolution neural networks for mobile edge computing in transportation cyber physical systems},
  author={Zhou, Junhao and Dai, Hong-Ning and Wang, Hao},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={6},
  pages={1--20},
  year={2019},
  publisher={ACM New York, NY, USA}
} 
@article{Dai:2019:BDA:3362097.3337065,
 author = {Dai, Hong-Ning and Wong, Raymond Chi-Wing and Wang, Hao and Zheng, Zibin and Vasilakos, Athanasios V.},
 title = {Big Data Analytics for Large-scale Wireless Networks: Challenges and Opportunities},
 journal = {ACM Comput. Surv.},
 issue_date = {October 2019},
 volume = {52},
 number = {5},
 month = sep,
 year = {2019},
 issn = {0360-0300},
 pages = {99:1--99:36},
 articleno = {99},
 numpages = {36},
 acmid = {3337065},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Big data, machine learning, wireless networks},
} 
@article{Tan2017,
abstract = {As a separation process, a paste thickener produces underflow with a high solids concentration. Such underflow leads to a high rake torque which could cause serious operational problems such as underflow blockage, pumping problems and potential donut formation. In this work, a model predictive control approach has been developed to control the underflow solids concentration subject to operational constraints. State observers including linear and extended Kalman filters are studied to determine a cost-effective approach to estimating the solids concentration profile in the paste thickener, which is important for thickener control. A rake torque model is validated with industrial plant data. By utilising the monotonic property of the rake torque model, a linear model predictive control (MPC) approach is developed to deal with the nonlinear constrained control problem. Using an industrial paste thickener as a case study, simulation results demonstrates that the proposed control approach, including an extended Kalman filter, can effectively regulate the underflow solids concentration within permissible operating windows, preventing rake lifting and pumping problems.},
author = {Tan, Chee Keong and Bao, Jie and Bickert, G{\"{o}}tz},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S0892687517300213-main.pdf:pdf},
issn = {08926875},
journal = {Minerals Engineering},
keywords = {Kalman filter,Model predictive control,Rake torque,Rheology,Sedimentation-consolidation model},
mendeley-groups = {pastethickener},
pages = {52--62},
publisher = {Elsevier Ltd},
title = {{A study on model predictive control in paste thickeners with rake torque constraint}},
volume = {105},
year = {2017}
}

@incollection{jeschke2017erratum,
  title={Erratum to: Industrial Internet of Things},
  author={Jeschke, Sabina and Brecher, Christian and Song, Houbing and Rawat, Danda B},
  booktitle={Industrial Internet of Things},
  pages={E1--E1},
  year={2017},
  publisher={Springer}
}

@article{bangemann2016integration,
  title={Integration of classical components into industrial cyber--physical systems},
  author={Bangemann, Thomas and Riedl, Matthias and Thron, Mario and Diedrich, Christian},
  journal={Proceedings of the IEEE},
  volume={104},
  number={5},
  pages={947--959},
  year={2016},
  publisher={IEEE}
}
@ARTICLE{Broersen2000,
author={P. M. T. {Broersen}},
journal={IEEE Transactions on Signal Processing},
title={Autoregressive model orders for Durbin's MA and ARMA estimators},
year={2000},
volume={48},
number={8},
pages={2454-2457},
keywords={moving average processes;autoregressive moving average processes;parameter estimation;autoregressive processes;sampling methods;autoregressive model orders;moving average parameters;autoregressive-moving average parameters;Durbin's methods;MA parameters;ARMA parameters;parameter estimation;linear regression theory;sample size;predicting AR model;long AR model;Predictive models;Parameter estimation;Linear regression;Computational modeling;Yield estimation;Maximum likelihood estimation;Physics;Minimization methods;Poles and zeros;Estimation theory},
ISSN={1941-0476},
month={Aug},}
@article{VALIPOUR2013433,
title = "Comparison of the ARMA, ARIMA, and the autoregressive artificial neural network models in forecasting the monthly inflow of Dez dam reservoir",
journal = "Journal of Hydrology",
volume = "476",
pages = "433 - 441",
year = "2013",
issn = "0022-1694",
author = "Mohammad Valipour and Mohammad Ebrahim Banihabib and Seyyed Mahmood Reza Behbahani",
keywords = "ARIMA, ARMA, Autoregressive artificial neural network, Dez dam, Forecast of dam reservoir inflow",
abstract = "Summary
The goal of the present research is forecasting the inflow of Dez dam reservoir by using Auto Regressive Moving Average (ARMA) and Auto Regressive Integrated Moving Average (ARIMA) models while increasing the number of parameters in order to increase the forecast accuracy to four parameters and comparing them with the static and dynamic artificial neural networks. In this research, monthly discharges from 1960 to 2007 were used. The statistics related to first 42years were used to train the models and the 5 past years were used to forecast. In ARMA and ARIMA models, the polynomial was derived respectively with four and six parameters to forecast the inflow. In the artificial neural network, the radial and sigmoid activity functions were used with several different neurons in the hidden layers. By comparing root mean square error (RMSE) and mean bias error (MBE), dynamic artificial neural network model with sigmoid activity function and 17 neurons in the hidden layer was chosen as the best model for forecasting inflow of the Dez dam reservoir. Inflow of the dam reservoir in the 12 past months shows that ARIMA model had a less error compared with the ARMA model. Static and Dynamic autoregressive artificial neural networks with activity sigmoid function can forecast the inflow to the dam reservoirs from the past 60months."
}
@article{Oh2010,
abstract = {In this paper, we present a new morphology-based homomorphic filtering technique for feature enhancement in medical images. The proposed method is based on decomposing an image into morphological subbands. The homomorphic filtering is performed using the morphological subbands. The differential evolution algorithm is applied to find an optimal gain and structuring element for each subband. Simulations show that the proposed filter improves the contrast of the features in medical images. {\textcopyright} 2010 ICROS, KIEE and Springer.},
author = {Oh, Jinsung and Hwang, Heesoo},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/s12555-010-0418-y.pdf:pdf},
issn = {15986446},
journal = {International Journal of Control, Automation and Systems},
keywords = {Differential evolution algorithm,homomorphic filter,image enhancement,morphological filter},
mendeley-groups = {machine learning},
number = {4},
pages = {857--861},
title = {{Feature enhancement of medical images using morphology-based homomorphic filter and differential evolution algorithm}},
volume = {8},
year = {2010}
}
@article{Langlois2019,
abstract = {The purpose of this paper is to extend existing mathematical models of tailings rheology and sedimentation to form a complete dynamic simulator. It is complete in the sense that it includes all important rheological variables for the development of multivariable control strategies and can be integrated with other stages of tailings management systems. This work extends a one-dimensional model for the dynamics of a flocculated suspension in a clarifier-thickener to include the discharge yield stress and particle size distribution in a manner that is computationally tractable. The paper also extends a static yield stress model to include the effect of particle size distribution, in a manner that is consistent with previous empirical and theoretical evidence. The dynamic simulator is validated through the simulation of a Proportional-Integral control strategy and proves to be a useful and flexible tool for the development of control strategies.},
author = {Langlois, Juan I. and Cipriano, Aldo},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S0892687518304771-main.pdf:pdf},
issn = {08926875},
journal = {Minerals Engineering},
keywords = {Dynamic simulation,Flocculation,Numerical simulation,Particle size distribution,Sedimentation-consolidation model,Yield stress},
mendeley-groups = {thickener},
pages = {131--139},
publisher = {Elsevier},
title = {{Dynamic modeling and simulation of tailing thickener units for the development of control strategies.}},
volume = {131},
year = {2019}
}
@article{Xiao2020,
abstract = {As a method for extracting metals and their compounds, hydrometallurgy has the advantages of high comprehensive metal recovery rate, low environmental pollution, and easier production process. The intensive washing process is a key process in the hydrometallurgical process, and the underflow concentration is a key indicator for measuring the quality of the concentrated washing process. In this paper, after analyzing the characteristics of the thick washing process, the hybrid model combining mechanism modeling and error compensation model based on EDO-TELM (three hidden layers Extreme Learning Machine optimized with Entire Distribution Optimization algorithm) is used to achieve accurate measurement of the underflow concentration in the dense washing process. The hybrid model uses the improved EDO-TELM algorithm as an error compensation model to compensate the error of the un-modeled part of the mechanism model, and gives a reasonable estimate of the uncertain part of the model, which theoretically reduce the prediction error of the model. The Matlab simulation results show that the prediction error of the hybrid model is significantly lower than that of the mechanism model and the data model, and can be adapted to the measurement needs of the industrial site.},
author = {Xiao, Dong and Xie, Hongfei and Jiang, Longqiang and Le, Ba Tuan and Wang, Jichun and Liu, Chong Min and Li, Hongzong},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Research on a method for predicting the underflow concentration of a thickener based on the hybrid model.pdf:pdf},
issn = {1997003X},
journal = {Engineering Applications of Computational Fluid Mechanics},
keywords = {EDO,Hydrometallurgy,TELM,hybrid model,mechanism model,underflow concentration},
mendeley-groups = {thickener},
number = {1},
pages = {13--26},
title = {{Research on a method for predicting the underflow concentration of a thickener based on the hybrid model}},
volume = {14},
year = {2020}
}
@article{Tan2015,
abstract = {Paste thickeners have attracted significant interest from mining industry due to its higher dewatering ability as compared to conventional or high rate thickeners. However, the underflow solids concentration, which is an important process variable of thickeners, is often poorly regulated. In this article, a dynamic model based on sedimentation-consolidation theory is adopted and validated using industrial plant data. Based on this model, control studies have been carried out to explore approaches to address a number of difficulties in current industrial operation. An extended Kalman filter is developed to estimate the compressibility parameter of the feed (coal tailing). As a key process parameter, coal tailing compressibility plays a significant role in thickener dynamics, but is time-varying and difficult to measure. Potential improvements of process operation by implementing model predictive control (MPC) are investigated. Simulation studies show that the proposed control can deliver a higher underflow solids concentration and a better regulated underflow removal rate than the existing operation. It is also demonstrated that taking into account the "future" time-varying input constraints in the MPC algorithm can help overcome the current control difficulty caused by co-disposal of coal tailing and coarse reject.},
author = {Tan, Chee Keong and Setiawan, Ridwan and Bao, Jie and Bickert, G{\"{o}}tz},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S0959152415000293-main.pdf:pdf},
issn = {09591524},
journal = {Journal of Process Control},
keywords = {Kalman filter Time-varying constraints,Mineral processing,Model predictive control,Sedimentation-consolidation model},
mendeley-groups = {thickener},
pages = {1--8},
publisher = {Elsevier Ltd},
title = {{Studies on parameter estimation and model predictive control of paste thickeners}},
volume = {28},
year = {2015}
}
@article{Zhang2018,
abstract = {Industrial Internet of Things (IIoT) is producing massive data which are valuable for knowing running status of the underlying equipment. However, these data involve various operation events that span some time, which raise questions on how to model long memory of states, and how to predict the running status based on historical data accurately. This paper aims to develop a method of: (1) analyzing equipment working condition based on the sensed data; (2) building a prediction model for working status forecasting and designing a deep neural network model to predict equipment running data; and (3) improving the prediction accuracy by systematic feature engineering and optimal hyperparameter searching. We evaluate our method with real-world monitoring data collected from 33 sensors of a main pump in a power station for three months. The model achieves less root mean square error than that of autoregressive integrated moving average model. Our method is applicable to general IIoT equipment for analyzing time series data and forecasting operation status.},
author = {Zhang, Weishan and Guo, Wuwu and Liu, Xin and Liu, Yan and Zhou, Jiehan and Li, Bo and Lu, Qinghua and Yang, Su},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/08335287.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {LSTM model,Time series prediction,industry Internet of Things,power equipment},
mendeley-groups = {time series},
pages = {23551--23560},
title = {{LSTM-Based Analysis of Industrial IoT Equipment}},
volume = {6},
year = {2018}
}
@article{Elman1990,
author = {Elman, Jeffrey L.},
title = {Finding Structure in Time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
year = {1990}
}
@article{Yuan2020,
abstract = {This paper focuses on the time series prediction problem for underflow concentration of deep cone thickener. It is commonly used in the industrial sedimentation process. In this paper, we introduce a dual attention neural network method to model both spatial and temporal features of the data collected from multiple sensors in the thickener to predict underflow concentration. The concentration is the key factor for future mining process. This model includes encoder and decoder. Their function is to capture spatial and temporal importance separately from input data, and output more accurate prediction. We also consider the domain knowledge in modeling process. Several supplementary constructed features are examined to enhance the final prediction accuracy in addition to the raw data from sensors. To test the feasibility and efficiency of this method, we select an industrial case based on Industrial Internet of Things (IIoT). This Tailings Thickener is from FLSmidth with multiple sensors. The comparative results support this method has favorable prediction accuracy, which is more than 10{\%} lower than other time series prediction models in some common error indices. We also try to interpret our method with additional ablation experiments for different features and attention mechanisms. By employing mean absolute error index to evaluate the models, experimental result reports that enhanced features and dual-attention modules reduce error of fitting {\~{}}5{\%} and {\~{}}11{\%}, respectively.},
author = {Yuan, Zhaolin and Hu, Jinlong and Wu, Di and Ban, Xiaojuan},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/A Dual-Attention Recurrent Neural Network Method for Deep Cone Thickener Underflow Concentration Prediction.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Cone thickener,Dual-attention,Industrial internet of things (IIoT),Spatio-temporal relationship,Time series prediction},
mendeley-groups = {myself},
number = {5},
pages = {1--18},
pmid = {32110906},
title = {{A dual-attention recurrent neural network method for deep cone thickener underflow concentration prediction}},
volume = {20},
year = {2020}
}

@ARTICLE{2020arXiv200207526Y,
       author = {{Yang}, Shuoheng and {Wang}, Yuxin and {Chu}, Xiaowen},
        title = "{A Survey of Deep Learning Techniques for Neural Machine Translation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2020,
        month = feb,
          eid = {arXiv:2002.07526},
        pages = {arXiv:2002.07526},
archivePrefix = {arXiv},
       eprint = {2002.07526},
 primaryClass = {cs.CL},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{Geffner2018,
abstract = {During the 60s and 70s, AI researchers explored intuitions about intelligence by writing programs that displayed intelligent behavior. Many good ideas came out from this work but programs written by hand were not robust or general. After the 80s, research increasingly shifted to the development of learners capable of inferring behavior and functions from experience and data, and solvers capable of tackling well-defined but intractable models like SAT, classical planning, Bayesian networks, and POMDPs. The learning approach has achieved considerable success but results in black boxes that do not have the flexibility, transparency, and generality of their model-based counterparts. Model-based approaches, on the other hand, require models and scalable algorithms. Model-free learners and model-based solvers have close parallels with Systems 1 and 2 in current theories of the human mind: the first, a fast, opaque, and inflexible intuitive mind; the second, a slow, transparent, and flexible analytical mind. In this paper, I review developments in AI and draw on these theories to discuss the gap between model-free learners and model-based solvers, a gap that needs to be bridged in order to have intelligent systems that are robust and general.},
archivePrefix = {arXiv},
arxivId = {1806.02308},
author = {Geffner, Hector},
eprint = {1806.02308},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1806.02308.pdf:pdf},
isbn = {9780999241127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
mendeley-groups = {RL{\_}control},
pages = {10--17},
title = {{Model-free, model-based, and general intelligence}},
volume = {2018-July},
year = {2018}
}
@incollection{GUIDOLIN2018113,
title = "Chapter 4 - Unit Roots and Cointegration",
editor = "Massimo Guidolin and Manuela Pedio",
booktitle = "Essentials of Time Series for Financial Applications",
publisher = "Academic Press",
pages = "113 - 149",
year = "2018",
isbn = "978-0-12-813409-2",
author = "Massimo Guidolin and Manuela Pedio",
keywords = "Nonstationarity, unit root, deterministic trend, cointegration, vector error-correction model, common stochastic trends",
abstract = "This chapter investigates the consequences of nonstationarity (in the form of unit roots in the assumed ARMA representation of a time series) for the econometric methodologies that have been developed in Chapters 1–3. Section 4.1 defines unit root processes and explains what it means to detrend such processes. Section 4.2 gives information about problems caused by the use of nonstationary, trending series in regression analysis. Section 4.3 describes the tests that can be used to verify the presence of a unit root in a series, in particular the Dickey–Fuller and the augmented Dickey–Fuller tests, along with those proposed by Philips and Perron and by Kwiatkowski, Phillips, Schmidt, and Shin. Finally, Section 4.4 introduces the concept of cointegration; it explains how to test for the presence of cointegrated variables, and the fact that the presence of cointegration always implies a vector error-correction model representation."
}% 

@article{kelly2020,
abstract = {Differential equations parameterized by neural networks become expensive to solve numerically as training progresses. We propose a remedy that encourages learned dynamics to be easier to solve. Specifically, we introduce a differentiable surrogate for the time cost of standard numerical solvers, using higher-order derivatives of solution trajectories. These derivatives are efficient to compute with Taylor-mode automatic differentiation. Optimizing this additional objective trades model performance against the time cost of solving the learned dynamics. We demonstrate our approach by training substantially faster, while nearly as accurate, models in supervised classification, density estimation, and time-series modelling tasks.},
archivePrefix = {arXiv},
arxivId = {2007.04504},
author = {Kelly, Jacob and Bettencourt, Jesse and Johnson, Matthew James and Duvenaud, David},
eprint = {2007.04504},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/NeurIPS-2020-learning-differential-equations-that-are-easy-to-solve-Paper.pdf:pdf},
journal = {arXiv},
mendeley-groups = {machine learning},
title = {{Learning differential equations that are easy to solve}},
volume = {3},
year = {2020}
}
@article{poli2020,
abstract = {The infinite-depth paradigm pioneered by Neural ODEs has launched a renaissance in the search for novel dynamical system-inspired deep learning primitives; however, their utilization in problems of non-trivial size has often proved impossible due to poor computational scalability. This work paves the way for scalable Neural ODEs with time-to-prediction comparable to traditional discrete networks. We introduce hypersolvers, neural networks designed to solve ODEs with low overhead and theoretical guarantees on accuracy. The synergistic combination of hypersolvers and Neural ODEs allows for cheap inference and unlocks a new frontier for practical application of continuous-depth models. Experimental evaluations on standard benchmarks, such as sampling for continuous normalizing flows, reveal consistent pareto efficiency over classical numerical methods.},
archivePrefix = {arXiv},
arxivId = {2007.09601},
author = {Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},
eprint = {2007.09601},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/NeurIPS-2020-hypersolvers-toward-fast-continuous-depth-models-Paper.pdf:pdf},
mendeley-groups = {machine learning},
number = {NeurIPS},
title = {{Hypersolvers: Toward Fast Continuous-Depth Models}},
year = {2020}
}
@inproceedings{J2020,
  title={How to train your neural ODE},
  author={Finlay, Chris and Jacobsen, J{\"o}rn-Henrik and Nurbekyan, Levon and Oberman, Adam},
  booktitle={International Conference on Machine Learning},
  pages={3154--3164},
  year={2020},
  organization={PMLR}
}
@article{zhao2019online,
  title={Online reinforcement learning control algorithm for concentration of thickener underflow},
  author={Yuan, Zhao-Lin and He, Run-Zi and Yao, Chao and Li, Jia and Ban, Xiao-Juan and Li, Xiao-Rui},
  journal={Acta Automatica Sinica},
  volume={45},
  pages={1--15},
  year={2019}
}
@article{jay2020recent,
  title={Recent advances and prospects in industrial AI and applications},
  author={Jay, Lee and Li Xiang and Xu Yuan-Ming and Yang Shaojie and Sun Ke-Yi},
  journal={Acta Automatica Sinica},
  volume={46},
  number={10},
  pages={2031--2044},
  year={2020}
}
@article{tian2020development,
  title={Development directions of industrial artificial intelligence},
  author={Chai, Tian-You},
  journal={Acta Automatica Sinica},
  volume={46},
  number={10},
  pages={2005--2012},
  year={2020}
}
@article{mercere2011parameterization,
  title={Parameterization and identification of multivariable state-space systems: A canonical approach},
  author={Merc{\`e}re, Guillaume and Bako, Laurent},
  journal={Automatica},
  volume={47},
  number={8},
  pages={1547--1555},
  year={2011},
  publisher={Elsevier}
}
@article{werbos1990backpropagation,
  title={Backpropagation through time: what it does and how to do it},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}
@article{Rangapuram2018,
abstract = {We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and inter-pretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from large collection of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.},
author = {Rangapuram, Syama Sundar and Seeger, Matthias and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/8004-deep-state-space-models-for-time-series-forecasting.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {system{\_}identify,SEQ{\_}VAE},
number = {NeurIPS},
pages = {7785--7794},
title = {{Deep state space models for time series forecasting}},
volume = {2018-Decem},
year = {2018}
}
@article{ma2020data,
  title={Data augmentation in microscopic images for material data mining},
  author={Ma, Boyuan and Wei, Xiaoyan and Liu, Chuni and Ban, Xiaojuan and Huang, Haiyou and Wang, Hao and Xue, Weihua and Wu, Stephen and Gao, Mingfei and Shen, Qing and others},
  journal={npj Computational Materials},
  volume={6},
  number={1},
  pages={1--9},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{young2018recent,
  title={Recent trends in deep learning based natural language processing},
  author={Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  journal={ieee Computational intelligenCe magazine},
  volume={13},
  number={3},
  pages={55--75},
  year={2018},
  publisher={IEEE}
}
@article{voulodimos2018deep,
  title={Deep learning for computer vision: A brief review},
  author={Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
  journal={Computational intelligence and neuroscience},
  volume={2018},
  year={2018},
  publisher={Hindawi}
}
@article{ma2021sesf,
  title={Sesf-fuse: An unsupervised deep model for multi-focus image fusion},
  author={Ma, Boyuan and Zhu, Yu and Yin, Xiang and Ban, Xiaojuan and Huang, Haiyou and Mukeshimana, Michele},
  journal={Neural Computing and Applications},
  volume={33},
  number={11},
  pages={5793--5804},
  year={2021},
  publisher={Springer}
}
@article{DIAZ2021106760,
title = {Random forest model predictive control for paste thickening},
journal = {Minerals Engineering},
volume = {163},
pages = {106760},
year = {2021},
issn = {0892-6875},
author = {Pablo Diaz and Juan C. Salas and Aldo Cipriano and Felipe Núñez},
keywords = {Paste thickening, Model predictive control, Random forest, Machine learning},
abstract = {As processes involved in mineral processing operations increase their complexity, automation and control become critical to ensure an economically viable and environmentally sustainable operation. In the context of modern mineral processing, paste thickening stands out as a relatively new method for producing high density slurries that has proven challenging for standard control algorithms. In this setting, the use of machine-learning-based models within a predictive control strategy arises as an appealing alternative. This work presents a Random Forest Model Predictive Control scheme for paste thickening based on a purely data-driven approach for modeling and evolutionary strategies for solving the associated optimization problem. Results show that the proposed strategy outperforms conventional predictive control both qualitatively and quantitatively.}
}
@INPROCEEDINGS{9429523,
  author={Oulhiq, Ridouane and Benjelloun, Khalid and Kali, Yassine and Saad, Maarouf},
  booktitle={2021 18th International Multi-Conference on Systems, Signals   Devices (SSD)}, 
  title={Identification and Control of an Industrial Thickener Using Historical Data}, 
  year={2021},
  volume={},
  number={},
  pages={915-920}
  }
  
  
@phdthesis{hazlin2020solving,
  title={Solving first order ordinary differential equation using adaptive Runge-Kutta method},
  author={Hazlin, Syauqina Nadia},
  year={2020},
  school={Universiti Teknologi MARA}
}

@article{Wu2020,
author = {Wu, Sifan and Xiao, Xi and Ding, Qianggang and Zhao, Peilin and Wei, Ying and Huang, Junzhou},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {17105--17115},
title = {{Adversarial Sparse Transformer for Time Series Forecasting}},
volume = {33},
year = {2020}
}
@article{liu2017survey,
  title={A survey of deep neural network architectures and their applications},
  author={Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E},
  journal={Neurocomputing},
  volume={234},
  pages={11--26},
  year={2017},
  publisher={Elsevier}
}
@article{luo2017inherently,
  title={An inherently nonnegative latent factor model for high-dimensional and sparse matrices from industrial applications},
  author={Luo, Xin and Zhou, MengChu and Li, Shuai and Shang, MingSheng},
  journal={IEEE Transactions on Industrial Informatics},
  volume={14},
  number={5},
  pages={2011--2022},
  year={2017},
  publisher={IEEE}
}
@article{luo2017incorporation,
  title={Incorporation of efficient second-order solvers into latent factor models for accurate prediction of missing QoS data},
  author={Luo, Xin and Zhou, MengChu and Li, Shuai and Xia, Yunni and You, Zhu-Hong and Zhu, QingSheng and Leung, Hareton},
  journal={IEEE transactions on cybernetics},
  volume={48},
  number={4},
  pages={1216--1228},
  year={2017},
  publisher={IEEE}
}
@article{wu2020data,
  title={A data-characteristic-aware latent factor model for web services QoS prediction},
  author={Wu, Di and Luo, Xin and Shang, Mingsheng and He, Yi and Wang, Guoyin and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2020},
  publisher={IEEE}
}
@ARTICLE{8681080,
  author={Lu, Huiyan and Jin, Long and Luo, Xin and Liao, Bolin and Guo, Dongsheng and Xiao, Lin},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={RNN for Solving Perturbed Time-Varying Underdetermined Linear System With Double Bound Limits on Residual Errors and State Variables}, 
  year={2019},
  volume={15},
  number={11},
  pages={5931-5942}
  }
  
@ARTICLE{8945486,
  author={Kebria, Parham M. and Khosravi, Abbas and Salaken, Syed Moshfeq and Nahavandi, Saeid},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Deep imitation learning for autonomous vehicles based on convolutional neural networks}, 
  year={2020},
  volume={7},
  number={1},
  pages={82-95},
  }
 @ARTICLE{9522017,
  author={Fei, Juntao and Liu, Lunhaojie},
  journal={IEEE Transactions on Industrial Electronics}, 
  title={Real-Time Nonlinear Model Predictive Control of Active Power Filter Using Self-Feedback Recurrent Fuzzy Neural Network Estimator}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  }
  
  @article{neu2021systematic,
  title={A systematic literature review on state-of-the-art deep learning methods for process prediction},
  author={Neu, Dominic A and Lahann, Johannes and Fettke, Peter},
  journal={Artificial Intelligence Review},
  pages={1--27},
  year={2021},
  publisher={Springer}
}
@article{larsson2002identification,
  title={Identification of continuous-time AR processes from unevenly sampled data},
  author={Larsson, Erik K and S{\"o}derstr{\"o}m, Torsten},
  journal={Automatica},
  volume={38},
  number={4},
  pages={709--718},
  year={2002},
  publisher={Elsevier}
}
@article{yin2020systematic,
  title={A systematic review of paste technology in metal mines for cleaner production in China},
  author={Yin, Shenghua and Shao, Yajian and Wu, Aixiang and Wang, Hongjiang and Liu, Xiaohui and Wang, Yong},
  journal={Journal of Cleaner Production},
  volume={247},
  pages={119590},
  year={2020},
  publisher={Elsevier}
}
@article{wu2020optimization,
  title={Optimization of flocculation and settling parameters of tailings slurry by response surface methodology},
  author={Wu, Aixiang and Ruan, Zhuen and B{\"u}rger, Raimund and Yin, Shenghua and Wang, Jiandong and Wang, Yong},
  journal={Minerals Engineering},
  volume={156},
  pages={106488},
  year={2020},
  publisher={Elsevier}
}
@article{li2019evaluation,
  title={Evaluation of short-term strength development of cemented backfill with varying sulphide contents and the use of additives},
  author={Li, Hong and Wu, Aixiang and Wang, Hongjiang},
  journal={Journal of environmental management},
  volume={239},
  pages={279--286},
  year={2019},
  publisher={Elsevier}
}
@article{christoffersen2001forecasting,
  title={Forecasting Non-Stationary Economic Time Series},
  author={Christoffersen, Peter F},
  journal={Journal of the American Statistical Association},
  volume={96},
  number={453},
  pages={347--347},
  year={2001},
  publisher={American Statistical Association}
}
@article{bogacki19893,
  title={A 3 (2) pair of Runge-Kutta formulas},
  author={Bogacki, Przemyslaw and Shampine, Lawrence F},
  journal={Applied Mathematics Letters},
  volume={2},
  number={4},
  pages={321--325},
  year={1989},
  publisher={Elsevier}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{duan2016,
  title={深度学习在控制领域的研究现状与展望},
  author={段艳杰 and 吕宜生 and 张杰 and 赵学亮 and 王飞跃},
  journal={自动化学报},
  volume={42},
  number={5},
  pages={643--654},
  year={2016}
}
@book{shumway2000time,
  title={Time series analysis and its applications},
  author={Shumway, Robert H and Stoffer, David S and Stoffer, David S},
  volume={3},
  year={2000},
  publisher={Springer}
}
@article{delgado1995dynamic,
  title={Dynamic recurrent neural network for system identification and control},
  author={Delgado, A and Kambhampati, C and Warwick, Kevin},
  journal={IEE Proceedings-Control Theory and Applications},
  volume={142},
  number={4},
  pages={307--314},
  year={1995},
  publisher={IET}
}
@article{zamarreno1998state,
  title={State space neural network. Properties and application},
  author={Zamarre{\~n}o, Jes{\'u}s M and Vega, Pastora},
  journal={Neural networks},
  volume={11},
  number={6},
  pages={1099--1112},
  year={1998},
  publisher={Elsevier}
}
@article{tan1996nonlinear,
  title={Nonlinear one-step-ahead control using neural networks: control strategy and stability design},
  author={Tan, Yonghong and Van Cauwenberghe, Achiel},
  journal={Automatica},
  volume={32},
  number={12},
  pages={1701--1706},
  year={1996},
  publisher={Elsevier}
}
@article{temeng1995model,
  title={Model predictive control of an industrial packed bed reactor using neural networks},
  author={Temeng, Kwaku O and Schnelle, Phillip D and McAvoy, Thomas J},
  journal={Journal of Process Control},
  volume={5},
  number={1},
  pages={19--27},
  year={1995},
  publisher={Elsevier}
}
@article{le2013system,
  title={System identification: new paradigms, challenges, and opportunities},
  author={Le-Yi, Wang and Wen-Xiao, Zhao},
  journal={Acta automatica sinica},
  volume={39},
  number={7},
  pages={933--942},
  year={2013},
  publisher={Elsevier}
}
@article{gevers2006personal,
  title={A personal view of the development of system identification: A 30-year journey through an exciting field},
  author={Gevers, Michel},
  journal={IEEE Control systems magazine},
  volume={26},
  number={6},
  pages={93--105},
  year={2006},
  publisher={IEEE}
}
@article{ljung2008perspectives,
  title={Perspectives on system identification},
  author={Ljung, Lennart},
  journal={IFAC Proceedings Volumes},
  volume={41},
  number={2},
  pages={7172--7184},
  year={2008},
  publisher={Elsevier}
}
@article{ljung2011four,
  title={Four encounters with system identification},
  author={Ljung, Lennart and Hjalmarsson, H{\aa}kan and Ohlsson, Henrik},
  journal={European Journal of Control},
  volume={17},
  number={5-6},
  pages={449--471},
  year={2011},
  publisher={Elsevier}
}
@article{funahashi1993approximation,
  title={Approximation of dynamical systems by continuous time recurrent neural networks},
  author={Funahashi, Ken-ichi and Nakamura, Yuichi},
  journal={Neural networks},
  volume={6},
  number={6},
  pages={801--806},
  year={1993},
  publisher={Elsevier}
}
@inproceedings{demeester2020system,
  title={System identification with time-aware neural sequence models},
  author={Demeester, Thomas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3757--3764},
  year={2020}
}
@article{Zhuang2020,
abstract = {The empirical performance of neural ordinary differential equations (NODEs) is significantly inferior to discrete-layer models on benchmark tasks (e.g. image classification). We demonstrate an explanation is the inaccuracy of existing gradient estimation methods: The adjoint method has numerical errors in reverse-mode integration; the naive method suffers from a redundantly deep computation graph. We propose the Adaptive Checkpoint Adjoint (ACA) method: ACA applies a trajectory checkpoint strategy which records the forwardmode trajectory as the reverse-mode trajectory to guarantee accuracy; ACA deletes redundant components for shallow computation graphs; and ACA supports adaptive solvers. On image classification tasks, compared with the adjoint and naive method, ACA achieves half the error rate in half the training time; NODE trained with ACA outperforms ResNet in both accuracy and test-retest reliability. On time-series modeling, ACA outperforms competing methods. Furthermore, NODE with ACA can incorporate physical knowledge to achieve better accuracy.},
archivePrefix = {arXiv},
arxivId = {2006.02493},
author = {Zhuang, Juntang and Dvornek, Nicha and Li, Xiaoxiao and Tatikonda, Sekhar and Papademetris, Xenophon and Duncan, James},
eprint = {2006.02493},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/zhuang20a.pdf:pdf},
isbn = {9781713821120},
journal = {37th International Conference on Machine Learning, ICML 2020},
mendeley-groups = {ODE,DE},
pages = {11575--11585},
title = {{Adaptive checkpoint adjoint method for gradient estimation in neural ODE}},
volume = {PartF16814},
year = {2020}
}
@article{Oganesyan2020,
abstract = {Stochastic regularization of neural networks (e.g. dropout) is a wide-spread technique in deep learning that allows for better generalization. Despite its success, continuous-time models, such as neural ordinary differential equation (ODE), usually rely on a completely deterministic feed-forward operation. This work provides an empirical study of stochastically regularized neural ODE on several image-classification tasks (CIFAR-10, CIFAR-100, TinyImageNet). Building upon the formalism of stochastic differential equations (SDEs), we demonstrate that neural SDE is able to outperform its deterministic counterpart. Further, we show that data augmentation during the training improves the performance of both deterministic and stochastic versions of the same model. However, the improvements obtained by the data augmentation completely eliminate the empirical gains of the stochastic regularization, making the difference in the performance of neural ODE and neural SDE negligible.},
archivePrefix = {arXiv},
arxivId = {2002.09779},
author = {Oganesyan, Viktor and Volokhova, Alexandra and Vetrov, Dmitry},
eprint = {2002.09779},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/stochasticity_in_neural_odes_an_empirical_study.pdf:pdf},
issn = {23318422},
journal = {arXiv},
mendeley-groups = {SDE},
pages = {1--9},
title = {{Stochasticity in Neural ODEs: An Empirical Study}},
year = {2020}
}

@article{Ghosh2020,
abstract = {Training Neural Ordinary Differential Equations (ODEs) is often computationally expensive. Indeed, computing the forward pass of such models involves solving an ODE which can become arbitrarily complex during training. Recent works have shown that regularizing the dynamics of the ODE can partially alleviate this. In this paper we propose a new regularization technique: randomly sampling the end time of the ODE during training. The proposed regularization is simple to implement, has negligible overhead and is effective across a wide variety of tasks. Further, the technique is orthogonal to several other methods proposed to regularize the dynamics of ODEs and as such can be used in conjunction with them. We show through experiments on normalizing flows, time series models and image recognition that the proposed regularization can significantly decrease training time and even improve performance over baseline models.},
archivePrefix = {arXiv},
arxivId = {2006.10711},
author = {Ghosh, Arnab and Torr, Philip H.S. and Behl, Harkirat Singh and Namboodiri, Vinay},
eprint = {2006.10711},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Ghosh et al. - 2020 - STEER Simple Temporal Regularization For Neural ODEs.pdf:pdf},
journal = {arXiv},
mendeley-groups = {VAE},
number = {NeurIPS},
title = {{STEER: Simple Temporal Regularization For Neural ODEs}},
year = {2020}
}
@InProceedings{OISR,
  author    = {Xiangyu He and Zitao Mo and Peisong Wang and Yang Liu and Mingyuan Yang and Jian Cheng},
  title     = {ODE-inspired Network Design for Single Image Super-Resolution},
  booktitle = {2019 {IEEE} Conference on Computer Vision and Pattern Recognition},
  month     = {July},
  year      = {2019}
}

@article{Huang2020,
abstract = {Many real-world systems, such as moving planets, can be considered as multi-agent dynamic systems, where objects interact with each other and co-evolve along with the time. Such dynamics is usually difficult to capture, and understanding and predicting the dynamics based on observed trajectories of objects become a critical research problem in many domains. Most existing algorithms, however, assume the observations are regularly sampled and all the objects can be fully observed at each sampling time, which is impractical for many applications. In this paper, we propose to learn system dynamics from irregularly-sampled partial observations with underlying graph structure for the first time. To tackle the above challenge, we present LG-ODE, a latent ordinary differential equation generative model for modeling multi-agent dynamic system with known graph structure. It can simultaneously learn the embedding of high dimensional trajectories and infer continuous latent system dynamics. Our model employs a novel encoder parameterized by a graph neural network that can infer initial states in an unsupervised way from irregularly-sampled partial observations of structural objects and utilizes neural ODE to infer arbitrarily complex continuous-time latent dynamics. Experiments on motion capture, spring system, and charged particle datasets demonstrate the effectiveness of our approach.},
archivePrefix = {arXiv},
arxivId = {2011.03880},
author = {Huang, Zijie and Sun, Yizhou and Wang, Wei},
eprint = {2011.03880},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/2011.03880.pdf:pdf},
issn = {23318422},
journal = {arXiv},
mendeley-groups = {VAE},
number = {NeurIPS},
pages = {1--14},
title = {{Learning continuous system dynamics from irregularly-sampled partial observations}},
year = {2020}
}
@inproceedings{morrill2021neural,
  title={Neural rough differential equations for long time series},
  author={Morrill, James and Salvi, Cristopher and Kidger, Patrick and Foster, James},
  booktitle={International Conference on Machine Learning},
  pages={7829--7838},
  year={2021},
  organization={PMLR}
}
@inproceedings{Yildiz2021,
author = {Yildiz, Cagatay and Heinonen, Markus and L{\"{a}}hdesm{\"{a}}ki, Harri},
booktitle = {International Conference on Machine Learning},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Heinonen - 2020 - Continuous-Time Model-Based Reinforcement Learning.pdf:pdf},
isbn = {2640-3498},
mendeley-groups = {RL_control},
pages = {12009--12018},
publisher = {PMLR},
title = {{Continuous-time Model-based Reinforcement Learning}},
year = {2021}
}
@inproceedings{li2020scalable,
  title={Scalable gradients for stochastic differential equations},
  author={Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky TQ and Duvenaud, David},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3870--3882},
  year={2020},
  organization={PMLR}
}
@article{morrill2021online,
  title={Neural Controlled Differential Equations for Online Prediction Tasks},
  author={Morrill, James and Kidger, Patrick and Yang, Lingyi and Lyons, Terry},
  journal={arXiv preprint arXiv:2106.11028},
  year={2021}
}
@article{ayed2019learning,
  title={Learning dynamical systems from partial observations},
  author={Ayed, Ibrahim and de B{\'e}zenac, Emmanuel and Pajot, Arthur and Brajard, Julien and Gallinari, Patrick},
  journal={arXiv preprint arXiv:1902.11136},
  year={2019}
}
@article{lechner2020learning,
  title={Learning long-term dependencies in irregularly-sampled time series},
  author={Lechner, Mathias and Hasani, Ramin},
  journal={arXiv preprint arXiv:2006.04418},
  year={2020}
}
@article{Grathwohl2019,
abstract = {Reversible generative models map points from a simple distribution to a complex distribution through an easily invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, improving the state-of-the-art among exact likelihood methods with efficient sampling.},
archivePrefix = {arXiv},
arxivId = {1810.01367},
author = {Grathwohl, Will and Chen, Ricky T.Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
eprint = {1810.01367},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Grathwohl et al. - 2019 - Ffjord Free-form continuous dynamics for scalable reversible generative models(2).pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
mendeley-groups = {machine learning},
pages = {1--13},
title = {{Ffjord: Free-form continuous dynamics for scalable reversible generative models}},
year = {2019}
}
@article{Jia2019,
abstract = {Many time series are effectively generated by a combination of deterministic continuous flows along with discrete jumps sparked by stochastic events. However, we usually do not have the equation of motion describing the flows, or how they are affected by jumps. To this end, we introduce Neural Jump Stochastic Differential Equations that provide a data-driven approach to learn continuous and discrete dynamic behavior, i.e., hybrid systems that both flow and jump. Our approach extends the framework of Neural Ordinary Differential Equations with a stochastic process term that models discrete events. We then model temporal point processes with a piecewise-continuous latent trajectory, where the discontinuities are caused by stochastic events whose conditional intensity depends on the latent state. We demonstrate the predictive capabilities of our model on a range of synthetic and real-world marked point process datasets, including classical point processes (such as Hawkes processes), awards on Stack Overflow, medical records, and earthquake monitoring.},
archivePrefix = {arXiv},
arxivId = {1905.10403},
author = {Jia, Junteng and Benson, Austin R.},
eprint = {1905.10403},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Neural Jump Stochastic Differential Equations.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {SDE},
number = {2018},
title = {{Neural jump stochastic differential equations}},
volume = {32},
year = {2019}
}
@article{Ainsworth2020,
abstract = {We study the estimation of policy gradients for continuous-time systems with known dynamics. By reframing policy learning in continuous-time, we show that it is possible construct a more efficient and accurate gradient estimator. The standard back-propagation through time estimator (BPTT) computes exact gradients for a crude discretization of the continuous-time system. In contrast, we approximate continuous-time gradients in the original system. With the explicit goal of estimating continuous-time gradients, we are able to discretize adaptively and construct a more efficient policy gradient estimator which we call the Continuous-Time Policy Gradient (CTPG). We show that replacing BPTT policy gradients with more efficient CTPG estimates results in faster and more robust learning in a variety of control tasks and simulators.},
archivePrefix = {arXiv},
arxivId = {2012.06684},
author = {Ainsworth, Samuel and Lowrey, Kendall and Thickstun, John and Harchaoui, Zaid and Srinivasa, Siddhartha},
eprint = {2012.06684},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/ainsworth21a.pdf:pdf},
keywords = {differentiable physics,neural odes,optimal control,policy,reinforcement learning},
mendeley-groups = {DE},
pages = {1--14},
title = {{Faster Policy Learning with Continuous-Time Gradients}},
volume = {144},
year = {2020}
}
@inproceedings{jia2019focnet,
  title={Focnet: A fractional optimal control network for image denoising},
  author={Jia, Xixi and Liu, Sanyang and Feng, Xiangchu and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6054--6063},
  year={2019}
}
@inproceedings{chen2021continuous,
  title={Continuous-time attention for sequential learning},
  author={Chen, Yi-Hsiang and Chien, Jen-Tzung},
  booktitle={Proc. of AAAI Conference on Aritificial Intelligence},
  year={2021}
}
@article{Liu2020,
archivePrefix = {arXiv},
arxivId = {2003.09229},
author = {Liu, Xuanqing and Yu, Hsiang Fu and Dhillon, Inderjit S. and Hsieh, Cho Jui},
eprint = {2003.09229},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/liu20n.pdf:pdf},
isbn = {9781713821120},
journal = {37th International Conference on Machine Learning, ICML 2020},
mendeley-groups = {Transformer},
pages = {6283--6291},
title = {{Learning to encode position for transformer with continuous dynamical model}},
volume = {PartF168147-9},
year = {2020}
}
@article{Vaswani2017,
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Vaswani et al. - 2017 - Attention is all you need.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {machine learning,Transformer},
number = {Nips},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-Decem},
year = {2017}
}
@article{Zhou2020,
abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism, which achieves $O(L \log L)$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.},
archivePrefix = {arXiv},
arxivId = {2012.07436},
author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
eprint = {2012.07436},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Zhou et al. - 2020 - Informer Beyond Efficient Transformer for Long Sequence Time-Series Forecasting(2).pdf:pdf},
mendeley-groups = {Transformer},
title = {{Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting}},
year = {2020}
}
@article{Wu2021,
abstract = {Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the \textit{long-term forecasting} problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Towards these challenges, we propose Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We go beyond the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.},
archivePrefix = {arXiv},
arxivId = {2106.13008},
author = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
eprint = {2106.13008},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/2106.13008.pdf:pdf},
mendeley-groups = {time series,Transformer},
pages = {1--11},
title = {{Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting}},
year = {2021}
}

@article{Ljung2020,
abstract = {Deep learning is a topic of considerable interest today. Since it deals with estimating - or learning - models, there are connections to the area of System Identification developed in the Automatic Control community. Such connections are explored and exploited in this contribution. It is stressed that common deep nets such as feedforward and cascadeforward nets are nonlinear ARX (NARX) models, and can thus be easily incorporated in System Identification code and practice. The case of LSTM nets is an example of NonLinear State-Space (NLSS) models.},
author = {Ljung, Lennart and Andersson, Carl and Tiels, Koen and Sch{\"{o}}n, Thomas B.},
doi = {10.1016/j.ifacol.2020.12.1329},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/ljung2020.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Bias/Variance Trade-off,Cascadeforwardnet,Deep nets,LSTM,Model structure,Model validation},
mendeley-groups = {system_identify},
number = {2},
pages = {1175--1181},
title = {{Deep learning and system identification}},
volume = {53},
year = {2020}
}

@article{Yin2014,
  title={Real-time implementation of fault-tolerant control systems with performance optimization},
  author={Yin, Shen and Luo, Hao and Ding, Steven X},
  journal={IEEE Transactions on Industrial Electronics},
  volume={61},
  number={5},
  pages={2402--2411},
  year={2013},
  publisher={IEEE}
}



@article{Kouro2009,
  title={Model predictive control—A simple and powerful method to control power converters},
  author={Kouro, Samir and Cort{\'e}s, Patricio and Vargas, Ren{\'e} and Ammann, Ulrich and Rodr{\'\i}guez, Jos{\'e}},
  journal={IEEE Transactions on industrial electronics},
  volume={56},
  number={6},
  pages={1826--1838},
  year={2008},
  publisher={IEEE}
}


@article{Dai2015,
  title={Data-driven optimization control for safety operation of hematite grinding process},
  author={Dai, Wei and Chai, Tianyou and Yang, Simon X},
  journal={IEEE Transactions on Industrial Electronics},
  volume={62},
  number={5},
  pages={2930--2941},
  year={2014},
  publisher={IEEE}
}

@article{Wang2016,
  title={Data-based adaptive critic designs for nonlinear robust optimal control with uncertain dynamics},
  author={Wang, Ding and Liu, Derong and Zhang, Qichao and Zhao, Dongbin},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume={46},
  number={11},
  pages={1544--1555},
  year={2015},
  publisher={IEEE}
}


@book{Sutton2018,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@book{F.L.LewisD.Vrabie2012,
  title={Optimal control},
  author={Lewis, Frank L and Vrabie, Draguna and Syrmos, Vassilis L},
  year={2012},
  publisher={John Wiley \& Sons}
}



@article{Prokhorov1997,
  title={Adaptive critic designs},
  author={Prokhorov, Danil V and Wunsch, Donald C},
  journal={IEEE transactions on Neural Networks},
  volume={8},
  number={5},
  pages={997--1007},
  year={1997},
  publisher={IEEE}
}

@article{Werbos2008,
  title={Foreword: ADP-The Key Direction for Future Research in Intelligent Control and Understanding Brain Intelligence.},
  author={Werbos, Paul J},
  journal={IEEE Trans. Syst. Man Cybern. Part B},
  volume={38},
  number={4},
  pages={898--900},
  year={2008}
}



@article{Duan:643,
  title={Deep learning for control: the state of the art and prospects},
  author={Duan, YJ and Lv, YS and Zhang, Jie and Zhao, XL and Wang, FY},
  journal={Acta Automatica Sinica},
  volume={42},
  number={5},
  pages={643--654},
  year={2016}
}


@article{Liu2015,
  title={Reinforcement learning design-based adaptive tracking control with less learning parameters for nonlinear discrete-time MIMO systems},
  author={Liu, Yan-Jun and Tang, Li and Tong, Shaocheng and Chen, CL Philip and Li, Dong-Juan},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={26},
  number={1},
  pages={165--176},
  year={2014},
  publisher={IEEE}
}



@article{LiuL2017,
  title={Adaptive fault-tolerant tracking control for MIMO discrete-time systems via reinforcement learning algorithm with less learning parameters},
  author={Liu, Lei and Wang, Zhanshan and Zhang, Huaguang},
  journal={IEEE Transactions on Automation Science and Engineering},
  volume={14},
  number={1},
  pages={299--313},
  year={2016},
  publisher={IEEE}
}



@article{XuX2017,
  title={Self-learning control using dual heuristic programming with global Laplacian eigenmaps},
  author={Xu, Xin and Yang, Huiyuan and Lian, Chuanqiang and Liu, Jiahang},
  journal={IEEE Transactions on Industrial Electronics},
  volume={64},
  number={12},
  pages={9517--9526},
  year={2017},
  publisher={IEEE}
}


@article{Wei2014,
  title={Adaptive dynamic programming for optimal tracking control of unknown nonlinear systems with application to coal gasification},
  author={Wei, Qinglai and Liu, Derong},
  journal={IEEE Transactions on Automation Science and Engineering},
  volume={11},
  number={4},
  pages={1020--1036},
  year={2013},
  publisher={IEEE}
}


@article{Jiang2018,
  title={Data-driven flotation industrial process operational optimal control based on reinforcement learning},
  author={Jiang, Yi and Fan, Jialu and Chai, Tianyou and Li, Jinna and Lewis, Frank L},
  journal={IEEE Transactions on Industrial Informatics},
  volume={14},
  number={5},
  pages={1974--1989},
  year={2017},
  publisher={IEEE}
}


@article{Jiang2019,
  title={Dual-rate operational optimal control for flotation industrial process with unknown operational model},
  author={Jiang, Yi and Fan, Jialu and Chai, Tianyou and Lewis, Frank L},
  journal={IEEE Transactions on Industrial Electronics},
  volume={66},
  number={6},
  pages={4587--4599},
  year={2018},
  publisher={IEEE}
}



@article{Modares2014,
  title={Integral reinforcement learning and experience replay for adaptive optimal control of partially-unknown constrained-input continuous-time systems},
  author={Modares, Hamidreza and Lewis, Frank L and Naghibi-Sistani, Mohammad-Bagher},
  journal={Automatica},
  volume={50},
  number={1},
  pages={193--202},
  year={2014},
  publisher={Elsevier}
}


@article{Mnih2013,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}


@article{Wang2012-GDHP,
  title={Optimal control of unknown nonaffine nonlinear discrete-time systems based on adaptive dynamic programming},
  author={Wang, Ding and Liu, Derong and Wei, Qinglai and Zhao, Dongbin and Jin, Ning},
  journal={Automatica},
  volume={48},
  number={8},
  pages={1825--1832},
  year={2012},
  publisher={Elsevier}
}



@article{Chai2016,
  title={An intelligent switching control for a mixed separation thickener process},
  author={Chai, Tianyou and Jia, Yao and Li, Haibo and Wang, Hong},
  journal={Control Engineering Practice},
  volume={57},
  pages={61--71},
  year={2016},
  publisher={Elsevier}
}



@article{WangLinyan2017,
  title={Dual-rate adaptive control for mixed separation thickening process using compensation signal based approach},
  author={Wang, Linyan and Jia, Yao and Chai, Tianyou and Xie, Wenfang},
  journal={IEEE Transactions on Industrial Electronics},
  volume={65},
  number={4},
  pages={3621--3632},
  year={2017},
  publisher={IEEE}
}

@phdthesis{WangMeng,
  title={矿浆中和沉降分离过程模型软件的研发},
  author={王猛},
  school={东北大学},
    year={2011},
}


@book{Tang2009,
  title={湿法冶金设备},
  author={唐谟堂},
  publisher={湿法冶金设备},
  year={2009},
}


@article{Wang330,
  title={混合选别浓密过程双速率智能切换控制},
  author={王琳岩 and 李健 and 贾瑶 and 柴天佑},
  journal={自动化学报},
  volume={44},
  number={2},
  pages={330--343},
  year={2018}
}



@article{Luo2016,
  title={Model-free optimal tracking control via critic-only Q-learning},
  author={Luo, Biao and Liu, Derong and Huang, Tingwen and Wang, Ding},
  journal={IEEE transactions on neural networks and learning systems},
  volume={27},
  number={10},
  pages={2134--2144},
  year={2016},
  publisher={IEEE}
}

@article{Padhi2006,
  title={A single network adaptive critic (SNAC) architecture for optimal control synthesis for a class of nonlinear systems},
  author={Padhi, Radhakant and Unnikrishnan, Nishant and Wang, Xiaohua and Balakrishnan, SN},
  journal={Neural Networks},
  volume={19},
  number={10},
  pages={1648--1660},
  year={2006},
  publisher={Elsevier}
}


@article{Xu2015,
  title={An intelligent control strategy for thickening process},
  author={Xu, Ning and Wang, Xu and Zhou, Junwu and Wang, Qingkai and Fang, Wen and Peng, Xiuyun},
  journal={International Journal of Mineral Processing},
  volume={142},
  pages={56--62},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{Ojeda2014,
  title={Intelligent control of an industrial thickener},
  author={Ojeda, Pablo and Bergh, Luis G and Torres, Luis},
  booktitle={2014 13th International Conference on Control Automation Robotics \& Vision (ICARCV)},
  pages={505--510},
  year={2014},
  organization={IEEE}
}


@article{Li2018,
  title={Off-policy Q-learning: Set-point design for optimizing dual-rate rougher flotation operational processes},
  author={Li, Jinna and Chai, Tianyou and Lewis, Frank L and Fan, Jialu and Ding, Zhengtao and Ding, Jinliang},
  journal={IEEE Transactions on Industrial Electronics},
  volume={65},
  number={5},
  pages={4092--4102},
  year={2017},
  publisher={IEEE}
}

@article{Xue2019,
  title={New methods for optimal operational control of industrial processes using reinforcement learning on two time scales},
  author={Xue, Wenqian and Fan, Jialu and Lopez, Victor G and Li, Jinna and Jiang, Yi and Chai, Tianyou and Lewis, Frank L},
  journal={IEEE Transactions on Industrial Informatics},
  volume={16},
  number={5},
  pages={3085--3099},
  year={2019},
  publisher={IEEE}
}
@article{Oulhiq2021,
abstract = {In the mining industry, thickeners are used to increase density of slurries by removing water. When a thickener is in operation, the underflow slurry density should follow a given setpoint and stay inside a boundary layer. In general, thickeners are manually controlled. However, industrial experience has shown that manual control is not sufficient. For this reason, an advanced process control strategy is proposed in this paper. On the one hand, using historical data, a linear dynamic model of the studied thickener is developed. In this regard, data is collected, prepared, and data informativity is studied to ensure model identifiability and interpretability. On the other hand, a model predictive control is designed to control the process by manipulating the feed and the underflow slurry flowrates. The simulation results show that the method can be successfully used to control the underflow slurry density in the presence of feed variations and unmeasured disturbances.},
author = {Oulhiq, Ridouane and Benjelloun, Khalid and Kali, Yassine and Saad, Maarouf},
doi = {10.1109/SSD52085.2021.9429523},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Identification_and_Control_of_an_Industrial_Thickener_Using_Historical_Data.pdf:pdf},
isbn = {9781665414937},
journal = {18th IEEE International Multi-Conference on Systems, Signals and Devices, SSD 2021},
keywords = {Dynamic modeling,Historical data,Model predictive control,Thickener},
pages = {915--920},
title = {{Identification and Control of an Industrial Thickener Using Historical Data}},
year = {2021}
}
@article{Member2019,
abstract = {This article presents a real implementtion of a neural network-based model predictive control scheme (NNMPC) to control an industrial paste thickener. The implementation is done over an Industrial Internet of Things (IIoT) platform designed using the seven layer reference model for IIoT systems. Modeling is achieved using an encoder-decoder with attention recurrent neural network, while MPC search is done using particle swarm optimization. An industrial evaluation is presented, which highlights the set-point tracking and disturbance rejection capabilities of the proposed NNMPC technique.},
author = {N{\'{u}}{\~{n}}ez, Felipe and Langarica, Sa{\'{u}}l and D{\'{i}}az, Pablo and Torres, Mario and Salas, Juan Carlos},
doi = {10.1109/TII.2019.2953275},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/N{\'{u}}{\~{n}}ez et al. - 2020 - Neural Network-Based Model Predictive Control of a Paste Thickener over an Industrial Internet Platform.pdf:pdf},
issn = {19410050},
journal = {IEEE Transactions on Industrial Informatics},
keywords = {Industrial Internet of Things (IIoT),Industrial automation,mineral processing,predictive control},
number = {4},
pages = {2859--2867},
title = {{Neural Network-Based Model Predictive Control of a Paste Thickener over an Industrial Internet Platform}},
volume = {16},
year = {2020}
}
@article{Langlois2019,
abstract = {The purpose of this paper is to extend existing mathematical models of tailings rheology and sedimentation to form a complete dynamic simulator. It is complete in the sense that it includes all important rheological variables for the development of multivariable control strategies and can be integrated with other stages of tailings management systems. This work extends a one-dimensional model for the dynamics of a flocculated suspension in a clarifier-thickener to include the discharge yield stress and particle size distribution in a manner that is computationally tractable. The paper also extends a static yield stress model to include the effect of particle size distribution, in a manner that is consistent with previous empirical and theoretical evidence. The dynamic simulator is validated through the simulation of a Proportional-Integral control strategy and proves to be a useful and flexible tool for the development of control strategies.},
author = {Langlois, Juan I. and Cipriano, Aldo},
doi = {10.1016/j.mineng.2018.11.006},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Dynamic modeling and simulation of tailing thickener units for the development of control strategies_.pdf:pdf},
issn = {08926875},
journal = {Minerals Engineering},
keywords = {Dynamic simulation,Flocculation,Numerical simulation,Particle size distribution,Sedimentation-consolidation model,Yield stress},
number = {May 2018},
pages = {131--139},
title = {{Dynamic modeling and simulation of tailing thickener units for the development of control strategies.}},
volume = {131},
year = {2019}
}
@article{Shin2020,
abstract = {Over the past few decades, advanced process control (APC) such as model predictive control (MPC) has been introduced to process industry to enhance its operational efficiency. For this, a linear model has been widely used to reduce the computational burden for iterative simulation and optimization over time, but it caused high inaccuracy of the control system. In this study, an artificial neural network (ANN) model was adopted instead of using the existing linearized model in order to increase the speed of optimization and accuracy of the model. For a case study, a depropanizer was modeled using Aspen HYSYS, and all feasible operation scenarios were considered to generate massive amounts of dynamic simulation data. Then, the accumulated data was implemented to the ANN for training, and it was tested. Once the verification was completed, the model was incorporated with an optimization algorithm in MPC system. For testing its performance, set point change and introduction of disturbances were applied to the model, and efficiency of the MPC was compared with the conventional control such as PID feedback control. The analysis results showed better performance (i.e., shorter settling time and rise time) of the MPC against the PID control. This methodology can be widely used in various types of control systems in the industry.},
author = {Shin, Yeonju and Smith, Robin and Hwang, Sungwon},
doi = {10.1016/j.jclepro.2020.124124},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S095965262034169X-main.pdf:pdf},
issn = {09596526},
journal = {Journal of Cleaner Production},
keywords = {Artificial neural networks,Model predictive control,Modeling,Optimization},
pages = {124124},
publisher = {Elsevier Ltd},
title = {{Development of model predictive control system using an artificial neural network: A case study with a distillation column}},
url = {https://doi.org/10.1016/j.jclepro.2020.124124},
volume = {277},
year = {2020}
}
@article{Diaz2021,
abstract = {As processes involved in mineral processing operations increase their complexity, automation and control become critical to ensure an economically viable and environmentally sustainable operation. In the context of modern mineral processing, paste thickening stands out as a relatively new method for producing high density slurries that has proven challenging for standard control algorithms. In this setting, the use of machine-learning-based models within a predictive control strategy arises as an appealing alternative. This work presents a Random Forest Model Predictive Control scheme for paste thickening based on a purely data-driven approach for modeling and evolutionary strategies for solving the associated optimization problem. Results show that the proposed strategy outperforms conventional predictive control both qualitatively and quantitatively.},
author = {Diaz, Pablo and Salas, Juan C. and Cipriano, Aldo and N{\'{u}}{\~{n}}ez, Felipe},
doi = {10.1016/j.mineng.2020.106760},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S089268752030580X-main.pdf:pdf},
issn = {08926875},
journal = {Minerals Engineering},
keywords = {Machine learning,Model predictive control,Paste thickening,Random forest},
number = {July 2020},
title = {{Random forest model predictive control for paste thickening}},
volume = {163},
year = {2021}
}
@article{Langarica2020,
abstract = {The Big Data revolution refers to using a large amount of data to improve decision making. In process control applications, the use of big data techniques has been restricted to complementing classical control schemes as model-based or PID approaches. This work focuses on a model-free purely data-driven control strategy known as Big Data approximating control (BDAC), which was recently introduced in the context of process control. In particular, this work proposes two modifications to the classical BDAC formulation and presents a real implementation of the enhanced BDAC technique to a real industrial paste thickener.},
author = {Langarica, Sa{\'{u}}l and N{\'{u}}{\~{n}}ez, Felipe},
doi = {10.1016/j.ifacol.2020.12.718},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S2405896320310363-main.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Big data,Cyber-physical systems,Industrial automation,Process control},
number = {2},
pages = {11944--11949},
title = {{Enhanced big data approximating control of an industrial paste thickener}},
volume = {53},
year = {2020}
}
@article{Member2021,
author = {Member, Student},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Neuroevolutive_Control_of_Industrial_Processes_Through_Mapping_Elites.pdf:pdf},
isbn = {2020212005},
number = {5},
pages = {3703--3713},
title = {{Neuroevolutive Control of Industrial}},
volume = {17},
year = {2021}
}
@article{Lesort2018,
abstract = {Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent's actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
archivePrefix = {arXiv},
arxivId = {1802.04181},
author = {Lesort, Timoth{\'{e}}e and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Goudou, Jean Frano̧is and Filliat, David},
doi = {10.1016/j.neunet.2018.07.006},
eprint = {1802.04181},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S0893608018302053-main(2).pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Disentanglement of control factors,Learning disentangled representations,Low dimensional embedding learning,Reinforcement learning,Robotics,State representation learning},
pages = {379--392},
pmid = {30268059},
title = {{State representation learning for control: An overview}},
volume = {108},
year = {2018}
}
@article{Marino2019,
abstract = {Artificial neural networks (ANNs) have been frequently used in
industrial applications to model complex systems. However, using
traditional ANNs for longterm planning tasks remains a challenge as they
lack the capability to model uncertainty. Process noise and
approximation errors cause ANN long-term estimations to deviate from the
real behavior of the system. Unlike traditional ANNs, stochastic models
provide a natural way to model uncertainty, providing estimations over a
range of several possible outcomes. This paper introduces a stochastic
modeling and planning approach using deep Bayesian neural networks
(DBNNs). We use DBNNs to learn a stochastic model of the system
dynamics. Planning is addressed as an open-loop trajectory optimization
problem. We present two approaches for learning the dynamics: using
single-step predictions and using multistep predictions. The advantages
of the proposed methodology are as follows. First, accurate long-term
estimations of the system state-trajectory probability distribution
without the need for expert knowledge of the dynamics. Second, improved
generalization and faster convergence rates in the trajectory
optimization task when using multistep predictions to train the model.
Third, viable for real-world applications since all expensive
optimizations are executed offline while using a reasonable number of
data samples. Testing is performed using challenging underactuated
benchmark problems: the Cartpole and the Acrobot. The presented
methodology successfully learns the swing-up maneuver using a relatively
small number of iterations, with less than 125 sampled trajectories, and
without any expert knowledge of the dynamics.},
author = {Marino, Daniel L. and Manic, Milos},
doi = {10.1109/tii.2019.2917520},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Modeling_and_Planning_Under_Uncertainty_Using_Deep_Neural_Networks.pdf:pdf},
issn = {1551-3203},
journal = {IEEE Transactions on Industrial Informatics},
number = {8},
pages = {4442--4454},
title = {{Modeling and Planning Under Uncertainty Using Deep Neural Networks}},
volume = {15},
year = {2019}
}
@article{Jaques2021,
abstract = {Learning low-dimensional latent state space dynamics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration. Notably, such proportional controlability also allows for robust path following from visual demonstrations using Dynamic Movement Primitives in the learned latent space.},
archivePrefix = {arXiv},
arxivId = {2006.01959},
author = {Jaques, Miguel and Burke, Michael and Hospedales, Timothy},
doi = {10.1109/CVPR46437.2021.00443},
eprint = {2006.01959},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Jaques_NewtonianVAE_Proportional_Control_and_Goal_Identification_From_Pixels_via_Physical_CVPR_2021_paper.pdf:pdf},
isbn = {9781665445092},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {SEQ_VAE},
pages = {4452--4461},
title = {{NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces}},
year = {2021}
}
@phdthesis{kidger2021,
    title={{O}n {N}eural {D}ifferential {E}quations},
    author={Patrick Kidger},
    year={2021},
    school={University of Oxford},
}

@article{chen2018neuralode,
  title={Neural Ordinary Differential Equations},
  author={Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  journal={Advances in Neural Information Processing Systems},
  year={2018}
}

@article{politorchdyn,
  title={TorchDyn: Implicit Models and Neural Numerical Methods in PyTorch},
  author={Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo and Ermon, Stefano}
}

@article{Yuan2022,
author = {Yuan, Zhaolin and Li, Xiaorui and Wu, Di and Ban, Xiaojuan and Wu, Naiqi and Dai, Hong-ning and Wang, Hao},
doi = {10.1109/JAS.2022.105464},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/发表版0210.pdf:pdf},
issn = {2329-9266},
journal = {IEEE/CAA Journal of Automatica Sinica},
mendeley-groups = {myself},
month = {apr},
number = {4},
pages = {686--698},
title = {{Continuous-Time Prediction of Industrial Paste Thickener System With Differential ODE-Net}},
url = {https://ieeexplore.ieee.org/document/9732304/},
volume = {9},
year = {2022}
}

@article{Che2018,
abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.},
archivePrefix = {arXiv},
arxivId = {1606.01865},
author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
doi = {10.1038/s41598-018-24271-9},
eprint = {1606.01865},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/s41598-018-24271-9.pdf:pdf},
isbn = {4159801824271},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--12},
pmid = {29666385},
publisher = {Springer US},
title = {{Recurrent Neural Networks for Multivariate Time Series with Missing Values}},
url = {http://dx.doi.org/10.1038/s41598-018-24271-9},
volume = {8},
year = {2018}
}
@article{Langlois2019,
abstract = {The purpose of this paper is to extend existing mathematical models of tailings rheology and sedimentation to form a complete dynamic simulator. It is complete in the sense that it includes all important rheological variables for the development of multivariable control strategies and can be integrated with other stages of tailings management systems. This work extends a one-dimensional model for the dynamics of a flocculated suspension in a clarifier-thickener to include the discharge yield stress and particle size distribution in a manner that is computationally tractable. The paper also extends a static yield stress model to include the effect of particle size distribution, in a manner that is consistent with previous empirical and theoretical evidence. The dynamic simulator is validated through the simulation of a Proportional-Integral control strategy and proves to be a useful and flexible tool for the development of control strategies.},
author = {Langlois, Juan I. and Cipriano, Aldo},
doi = {10.1016/j.mineng.2018.11.006},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Dynamic modeling and simulation of tailing thickener units for the development of control strategies_.pdf:pdf},
issn = {08926875},
journal = {Minerals Engineering},
keywords = {Dynamic simulation,Flocculation,Numerical simulation,Particle size distribution,Sedimentation-consolidation model,Yield stress},
number = {May 2018},
pages = {131--139},
title = {{Dynamic modeling and simulation of tailing thickener units for the development of control strategies.}},
volume = {131},
year = {2019}
}
@article{Langarica2020,
abstract = {The Big Data revolution refers to using a large amount of data to improve decision making. In process control applications, the use of big data techniques has been restricted to complementing classical control schemes as model-based or PID approaches. This work focuses on a model-free purely data-driven control strategy known as Big Data approximating control (BDAC), which was recently introduced in the context of process control. In particular, this work proposes two modifications to the classical BDAC formulation and presents a real implementation of the enhanced BDAC technique to a real industrial paste thickener.},
author = {Langarica, Sa{\'{u}}l and N{\'{u}}{\~{n}}ez, Felipe},
doi = {10.1016/j.ifacol.2020.12.718},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S2405896320310363-main.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Big data,Cyber-physical systems,Industrial automation,Process control},
number = {2},
pages = {11944--11949},
title = {{Enhanced big data approximating control of an industrial paste thickener}},
volume = {53},
year = {2020}
}
@article{Oulhiq2021,
abstract = {In the mining industry, thickeners are used to increase density of slurries by removing water. When a thickener is in operation, the underflow slurry density should follow a given setpoint and stay inside a boundary layer. In general, thickeners are manually controlled. However, industrial experience has shown that manual control is not sufficient. For this reason, an advanced process control strategy is proposed in this paper. On the one hand, using historical data, a linear dynamic model of the studied thickener is developed. In this regard, data is collected, prepared, and data informativity is studied to ensure model identifiability and interpretability. On the other hand, a model predictive control is designed to control the process by manipulating the feed and the underflow slurry flowrates. The simulation results show that the method can be successfully used to control the underflow slurry density in the presence of feed variations and unmeasured disturbances.},
author = {Oulhiq, Ridouane and Benjelloun, Khalid and Kali, Yassine and Saad, Maarouf},
doi = {10.1109/SSD52085.2021.9429523},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Identification_and_Control_of_an_Industrial_Thickener_Using_Historical_Data.pdf:pdf},
isbn = {9781665414937},
journal = {18th IEEE International Multi-Conference on Systems, Signals and Devices, SSD 2021},
keywords = {Dynamic modeling,Historical data,Model predictive control,Thickener},
pages = {915--920},
title = {{Identification and Control of an Industrial Thickener Using Historical Data}},
year = {2021}
}
@article{Jaques2021,
abstract = {Learning low-dimensional latent state space dynamics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration. Notably, such proportional controlability also allows for robust path following from visual demonstrations using Dynamic Movement Primitives in the learned latent space.},
archivePrefix = {arXiv},
arxivId = {2006.01959},
author = {Jaques, Miguel and Burke, Michael and Hospedales, Timothy},
doi = {10.1109/CVPR46437.2021.00443},
eprint = {2006.01959},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Jaques_NewtonianVAE_Proportional_Control_and_Goal_Identification_From_Pixels_via_Physical_CVPR_2021_paper.pdf:pdf},
isbn = {9781665445092},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4452--4461},
title = {{NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces}},
year = {2021}
}
@article{Shin2020,
abstract = {Over the past few decades, advanced process control (APC) such as model predictive control (MPC) has been introduced to process industry to enhance its operational efficiency. For this, a linear model has been widely used to reduce the computational burden for iterative simulation and optimization over time, but it caused high inaccuracy of the control system. In this study, an artificial neural network (ANN) model was adopted instead of using the existing linearized model in order to increase the speed of optimization and accuracy of the model. For a case study, a depropanizer was modeled using Aspen HYSYS, and all feasible operation scenarios were considered to generate massive amounts of dynamic simulation data. Then, the accumulated data was implemented to the ANN for training, and it was tested. Once the verification was completed, the model was incorporated with an optimization algorithm in MPC system. For testing its performance, set point change and introduction of disturbances were applied to the model, and efficiency of the MPC was compared with the conventional control such as PID feedback control. The analysis results showed better performance (i.e., shorter settling time and rise time) of the MPC against the PID control. This methodology can be widely used in various types of control systems in the industry.},
author = {Shin, Yeonju and Smith, Robin and Hwang, Sungwon},
doi = {10.1016/j.jclepro.2020.124124},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S095965262034169X-main.pdf:pdf},
issn = {09596526},
journal = {Journal of Cleaner Production},
keywords = {Artificial neural networks,Model predictive control,Modeling,Optimization},
pages = {124124},
publisher = {Elsevier Ltd},
title = {{Development of model predictive control system using an artificial neural network: A case study with a distillation column}},
url = {https://doi.org/10.1016/j.jclepro.2020.124124},
volume = {277},
year = {2020}
}
@article{Member2021,
author = {Member, Student},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Neuroevolutive_Control_of_Industrial_Processes_Through_Mapping_Elites.pdf:pdf},
isbn = {2020212005},
number = {5},
pages = {3703--3713},
title = {{Neuroevolutive Control of Industrial}},
volume = {17},
year = {2021}
}
@article{Li2018,
abstract = {To ensure undisrupted web-based services, operators need to closely monitor various KPIs (Key Performance Indicator, such as CPU usages, network throughput, page views, number of online users, and etc), detect anomalies in them, and trigger timely troubleshooting or mitigation. There can be hundreds of thousands to even millions of KPIs to be monitored, thus operators need automatic anomaly detection approaches. However, neither traditional statistical approaches nor supervised ensemble approaches satisfy this requirement in practice when facing large number of KPIs. A state-of-art unsupervised approach Donut offering promising results, but it is not a sequential model thus cannot deal with the time information related anomalies. Thus, in this paper we propose Bagel, a robust and unsupervised anomaly detection algorithm for KPI that can handle time information related anomalies, using CVAE to incorporate time information and dropout layer to avoid overfitting. Our experiments using real data from Internet companies show that, compared to Donut, Bagel improves the anomaly detection best F1-score by 0.08 to 0.43.},
author = {Li, Zeyan and Chen, Wenxiao and Pei, Dan},
doi = {10.1109/PCCC.2018.8710885},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Robust and Unsupervised KPI Anomaly Detection.pdf:pdf},
isbn = {9781538668085},
journal = {2018 IEEE 37th International Performance Computing and Communications Conference, IPCCC 2018},
title = {{Robust and Unsupervised KPI Anomaly Detection Based on Conditional Variational Autoencoder}},
year = {2018}
}
@article{Xu2018,
abstract = {To ensure undisrupted business, large Internet companies need to closely monitor various KPIs (e.g., Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed Donut, an unsupervised anomaly detection algorithm based on VAE. Thanks to a few of our key techniques, Donut greatly outperforms a state-of-arts supervised ensemble approach and a baseline VAE approach, and its best F-scores range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. We come up with a novel KDE interpretation of reconstruction for Donut, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation.},
archivePrefix = {arXiv},
arxivId = {1802.03903},
author = {Xu, Haowen and Chen, Wenxiao and Zhao, Nengwen and Li, Zeyan and Bu, Jiahao and Li, Zhihan and Liu, Ying and Zhao, Youjian and Pei, Dan and Feng, Yang and Chen, Jie and Wang, Zhaogang and Qiao, Honglin},
doi = {10.1145/3178876.3185996},
eprint = {1802.03903},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/2018 Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications.pdf:pdf},
isbn = {9781450356398},
journal = {The Web Conference 2018 - Proceedings of the World Wide Web Conference, WWW 2018},
keywords = {Anomaly detection,Seasonal KPI,Variational auto-encoder},
pages = {187--196},
title = {{Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications}},
volume = {2},
year = {2018}
}
@article{Marino2019,
abstract = {Artificial neural networks (ANNs) have been frequently used in
industrial applications to model complex systems. However, using
traditional ANNs for longterm planning tasks remains a challenge as they
lack the capability to model uncertainty. Process noise and
approximation errors cause ANN long-term estimations to deviate from the
real behavior of the system. Unlike traditional ANNs, stochastic models
provide a natural way to model uncertainty, providing estimations over a
range of several possible outcomes. This paper introduces a stochastic
modeling and planning approach using deep Bayesian neural networks
(DBNNs). We use DBNNs to learn a stochastic model of the system
dynamics. Planning is addressed as an open-loop trajectory optimization
problem. We present two approaches for learning the dynamics: using
single-step predictions and using multistep predictions. The advantages
of the proposed methodology are as follows. First, accurate long-term
estimations of the system state-trajectory probability distribution
without the need for expert knowledge of the dynamics. Second, improved
generalization and faster convergence rates in the trajectory
optimization task when using multistep predictions to train the model.
Third, viable for real-world applications since all expensive
optimizations are executed offline while using a reasonable number of
data samples. Testing is performed using challenging underactuated
benchmark problems: the Cartpole and the Acrobot. The presented
methodology successfully learns the swing-up maneuver using a relatively
small number of iterations, with less than 125 sampled trajectories, and
without any expert knowledge of the dynamics.},
author = {Marino, Daniel L. and Manic, Milos},
doi = {10.1109/tii.2019.2917520},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Modeling_and_Planning_Under_Uncertainty_Using_Deep_Neural_Networks.pdf:pdf},
issn = {1551-3203},
journal = {IEEE Transactions on Industrial Informatics},
number = {8},
pages = {4442--4454},
title = {{Modeling and Planning Under Uncertainty Using Deep Neural Networks}},
volume = {15},
year = {2019}
}
@article{Diaz2021,
abstract = {As processes involved in mineral processing operations increase their complexity, automation and control become critical to ensure an economically viable and environmentally sustainable operation. In the context of modern mineral processing, paste thickening stands out as a relatively new method for producing high density slurries that has proven challenging for standard control algorithms. In this setting, the use of machine-learning-based models within a predictive control strategy arises as an appealing alternative. This work presents a Random Forest Model Predictive Control scheme for paste thickening based on a purely data-driven approach for modeling and evolutionary strategies for solving the associated optimization problem. Results show that the proposed strategy outperforms conventional predictive control both qualitatively and quantitatively.},
author = {Diaz, Pablo and Salas, Juan C. and Cipriano, Aldo and N{\'{u}}{\~{n}}ez, Felipe},
doi = {10.1016/j.mineng.2020.106760},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S089268752030580X-main.pdf:pdf},
issn = {08926875},
journal = {Minerals Engineering},
keywords = {Machine learning,Model predictive control,Paste thickening,Random forest},
number = {July 2020},
title = {{Random forest model predictive control for paste thickening}},
volume = {163},
year = {2021}
}
@article{Li2018a,
abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
archivePrefix = {arXiv},
arxivId = {1803.02991},
author = {Li, Yingzhen and Mandt, Stephan},
eprint = {1803.02991},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/yingzhen18a.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Disentangled sequential autoencoder}},
year = {2018}
}
@article{VSDN_Liu2020,
abstract = {Learning continuous-time stochastic dynamics from sparse or irregular observations is a fundamental and essential problem for many real-world applications. However, for a given system whose latent states and observed data are high-dimensional, it is generally impossible to derive a precise continuous-time stochastic process to describe the system behaviors. To solve the above problem, we apply Variational Bayesian method and propose a flexible continuous-time framework named Variational Stochastic Differential Networks (VSDN), which can model high-dimensional nonlinear stochastic dynamics by deep neural networks. VSDN introduces latent states to modulate the estimated distribution and defines two practical methods to model the stochastic dependency between observations and the states. The first variant, which is called VSDN-VAE, incorporates sequential Variational Auto-Encoder (VAE) to efficiently model the distribution of the latent states. The second variant, called VSDN-SDE, further extends the model capacity of VSDN-VAE by learning a set of Stochastic Differential Equations (SDEs) to fully describe the state transitions. Through comprehensive experiments on symbolic MIDI and speech datasets, we show that VSDNs can accurately model the continuous-time dynamics and achieve remarkable performance on challenging tasks, including online prediction and sequence interpolation.},
archivePrefix = {arXiv},
arxivId = {2006.06145},
author = {Liu, Yingru and Wang, Xin and Xing, Yucheng and Jin, Di and Yang, Xuewen and Shi, Jing},
eprint = {2006.06145},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/2006.06145.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--13},
title = {{Learning continuous-time dynamics by Stochastic Differential Networks}},
year = {2020}
}
@article{Lee2019,
  title={Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model},
  author={Lee, Alex X and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={741--752},
  year={2020}
}
@article{Lesort2018,
abstract = {Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent's actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
archivePrefix = {arXiv},
arxivId = {1802.04181},
author = {Lesort, Timoth{\'{e}}e and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Goudou, Jean Frano̧is and Filliat, David},
doi = {10.1016/j.neunet.2018.07.006},
eprint = {1802.04181},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1-s2.0-S0893608018302053-main(2).pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Disentanglement of control factors,Learning disentangled representations,Low dimensional embedding learning,Reinforcement learning,Robotics,State representation learning},
pages = {379--392},
pmid = {30268059},
title = {{State representation learning for control: An overview}},
volume = {108},
year = {2018}
}
@article{Gedon,
archivePrefix = {arXiv},
arxivId = {arXiv:2003.14162v3},
author = {Gedon, Daniel and Wahlstr, Niklas},
eprint = {arXiv:2003.14162v3},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/2003.14162.pdf:pdf},
keywords = {black box modeling,deep learning,nonlinear system identification},
title = {{Deep State Space Models for Nonlinear System Identification}}
}
@article{Rubanova2019,
abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
archivePrefix = {arXiv},
arxivId = {1907.03907},
author = {Rubanova, Yulia and Chen, Ricky T.Q. and Duvenaud, David},
eprint = {1907.03907},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/NeurIPS-2019-latent-ordinary-differential-equations-for-irregularly-sampled-time-series-Paper.pdf:pdf;:Users/yuanzhaolin/Documents/Mendeley Desktop/supplement_updated.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{Latent ODEs for irregularly-sampled time series}},
volume = {32},
year = {2019}
}
@article{Li2020,
archivePrefix = {arXiv},
arxivId = {2001.01328},
author = {Li, Xuechen and {Leonard Wong}, Ting Kam and Chen, Ricky T.Q. and Duvenaud, David},
eprint = {2001.01328},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Li et al. - 2020 - Scalable gradients for stochastic differential equations.pdf:pdf;:Users/yuanzhaolin/Documents/Mendeley Desktop/li20i-supp.pdf:pdf},
journal = {arXiv},
title = {{Scalable gradients for stochastic differential equations}},
volume = {108},
year = {2020}
}


@inproceedings{Quaglino2019,
 author = {Alessio Quaglino and
Marco Gallieri and
Jonathan Masci and
Jan Koutn{\'{\i}}k},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/QuaglinoGMK20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {{SNODE:} Spectral Discretization of Neural ODEs for System Identification},
 url = {https://openreview.net/forum?id=Sye0XkBKvS},
 year = {2020}
}

@article{aastrom1971system,
  title={System identification—a survey},
  author={{\AA}str{\"o}m, Karl Johan and Eykhoff, Peter},
  journal={Automatica},
  volume={7},
  number={2},
  pages={123--162},
  year={1971},
  publisher={Elsevier}
}

@inproceedings{wang2017new,
  title={A new concept using lstm neural networks for dynamic system identification},
  author={Wang, Yu},
  booktitle={2017 American control conference (ACC)},
  pages={5324--5329},
  year={2017},
  organization={IEEE}
}
@article{ogunmolu2016nonlinear,
  title={Nonlinear systems identification using deep dynamic neural networks},
  author={Ogunmolu, Olalekan and Gu, Xuejun and Jiang, Steve and Gans, Nicholas},
  journal={arXiv preprint arXiv:1610.01439},
  year={2016}
}

@inproceedings{zhong2019symplectic,
  title={Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control},
  author={Zhong, Yaofeng Desmond and Dey, Biswadip and Chakraborty, Amit},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{lesort2018state,
  title={State representation learning for control: An overview},
  author={Lesort, Timoth{\'e}e and D{\'\i}az-Rodr{\'\i}guez, Natalia and Goudou, Jean-Franois and Filliat, David},
  journal={Neural Networks},
  volume={108},
  pages={379--392},
  year={2018},
  publisher={Elsevier}
}
@inproceedings{kingma2013auto,
 author = {Diederik P. Kingma and
Max Welling},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
 booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
 title = {Auto-Encoding Variational Bayes},
 url = {http://arxiv.org/abs/1312.6114},
 year = {2014}
}
@article{Fraccaro2016,
archivePrefix = {arXiv},
arxivId = {1605.07571},
author = {Fraccaro, Marco and S{\o}nderby, S{\o}ren Kaae and Paquet, Ulrich and Winther, Ole},
eprint = {1605.07571},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1605.07571.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {VAE},
pages = {2207--2215},
title = {{Sequential neural models with stochastic layers}},
year = {2016}
}
@article{Chung2015,
abstract = {In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02216v4},
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1506.02216v4},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/1506.02216v4.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {VAE},
pages = {2980--2988},
title = {{A recurrent latent variable model for sequential data}},
volume = {2015-Janua},
year = {2015}
}
@inproceedings{Karl2017,
 author = {Maximilian Karl and
Maximilian Soelch and
Justin Bayer and
Patrick van der Smagt},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/KarlSBS17.bib},
 booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Mon, 31 Aug 2020 01:00:00 +0200},
 title = {Deep Variational Bayes Filters: Unsupervised Learning of State Space
Models from Raw Data},
 url = {https://openreview.net/forum?id=HyTqHL5xg},
 year = {2017}
}
@inproceedings{yildiz2021continuous,
  title={Continuous-time Model-based Reinforcement Learning},
  author={Yildiz, Cagatay and Heinonen, Markus and L{\"a}hdesm{\"a}ki, Harri},
  booktitle={International Conference on Machine Learning},
  pages={12009--12018},
  year={2021},
  organization={PMLR}
}
@inproceedings{doerr2018probabilistic,
  title={Probabilistic recurrent state-space models},
  author={Doerr, Andreas and Daniel, Christian and Schiegg, Martin and Duy, Nguyen-Tuong and Schaal, Stefan and Toussaint, Marc and Sebastian, Trimpe},
  booktitle={International Conference on Machine Learning},
  pages={1280--1289},
  year={2018},
  organization={PMLR}
}
@inproceedings{deisenroth2011pilco,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages={465--472},
  year={2011}
}
@inproceedings{venkatraman2015improving,
  title={Improving multi-step prediction of learned time series models},
  author={Venkatraman, Arun and Hebert, Martial and Bagnell, J Andrew},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}
@article{Guo2019,
author = {Guo, Xinxin and Yan, Weisheng and Cui, Rongxin},
doi = {10.1109/tsmc.2019.2897221},
file = {:Users/yuanzhaolin/Documents/Mendeley Desktop/Guo, Yan, Cui - 2019 - Integral Reinforcement Learning-Based Adaptive NN Control for Continuous-Time Nonlinear MIMO Systems With Unknown.pdf:pdf},
issn = {2168-2216},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
mendeley-groups = {IRL},
pages = {1--10},
publisher = {IEEE},
title = {{Integral Reinforcement Learning-Based Adaptive NN Control for Continuous-Time Nonlinear MIMO Systems With Unknown Control Directions}},
volume = {PP},
year = {2019}
}

@ARTICLE{8709809,
  author={Yang, Hongyan and Yin, Shen},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Actuator and Sensor Fault Estimation for Time-Delay Markov Jump Systems With Application to Wheeled Mobile Manipulators}, 
  year={2020},
  volume={16},
  number={5},
  pages={3222-3232},
  doi={10.1109/TII.2019.2915668}}
@article{song2015almost,
  title={Almost sure stability of switching Markov jump linear systems},
  author={Song, Yang and Yang, Jie and Yang, Taicheng and Fei, Minrui},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={9},
  pages={2638--2643},
  year={2015},
  publisher={IEEE}
}
  @ARTICLE{9165930,
  author={Wang, Yu and He, Long and Jiang, Shan and Chow, Tommy W. S.},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Failure Prediction of Hard Disk Drives Based on Adaptive Rao–Blackwellized Particle Filter Error Tracking Method}, 
  year={2021},
  volume={17},
  number={2},
  pages={913-921},
  doi={10.1109/TII.2020.3016121}}
@InProceedings{pmlr-v120-jansch-porto20a,
  title = 	 {Policy Learning of MDPs with Mixed Continuous/Discrete Variables: A Case Study on  Model-Free Control of Markovian Jump Systems},
  author =       {Jansch-Porto, Joao Paulo and Hu, Bin and Dullerud, Geir},
  booktitle = 	 {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
  pages = 	 {947--957},
  year = 	 {2020},
  editor = 	 {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
  volume = 	 {120},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--11 Jun},
  publisher =    {PMLR},
}

  @article{song2015almost,
  title={Almost sure stability of switching Markov jump linear systems},
  author={Song, Yang and Yang, Jie and Yang, Taicheng and Fei, Minrui},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={9},
  pages={2638--2643},
  year={2015},
  publisher={IEEE}
}
@article{zadeh1956identification,
  title={On the identification problem},
  author={Zadeh, L},
  journal={IRE Transactions on Circuit Theory},
  volume={3},
  number={4},
  pages={277--281},
  year={1956},
  publisher={IEEE}
}
@article{jordan1992forward,
  title={Forward models: Supervised learning with a distal teacher},
  author={Jordan, Michael I and Rumelhart, David E},
  journal={Cognitive science},
  volume={16},
  number={3},
  pages={307--354},
  year={1992},
  publisher={Elsevier}
}
@inproceedings{silver2008sample,
  title={Sample-based learning and search with permanent and transient memories},
  author={Silver, David and Sutton, Richard S and M{\"u}ller, Martin},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={968--975},
  year={2008}
}
@inproceedings{werbos1989neural,
  title={Neural networks for control and system identification},
  author={Werbos, Paul J},
  booktitle={Proceedings of the 28th IEEE Conference on Decision and Control,},
  pages={260--265},
  year={1989},
  organization={IEEE}
}
@book{lin1992memory,
  title={Memory approaches to reinforcement learning in non-Markovian domains},
  author={Lin, Long-Ji and Mitchell, Tom M},
  year={1992},
  publisher={Citeseer}
}
@article{WANG2022111790,
title = {Model and data driven transient thermal system modelings for contained data centers},
journal = {Energy and Buildings},
volume = {258},
pages = {111790},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111790},
author = {Yewan Wang and Yiru Zhang and David Nörtershäuser and Stéphane {Le Masson} and Jean-Marc Menaud},
}
@article{balenzuela2022parameter,
  title={Parameter estimation for Jump Markov Linear Systems},
  author={Balenzuela, Mark P and Wills, Adrian G and Renton, Christopher and Ninness, Brett},
  journal={Automatica},
  volume={135},
  pages={109949},
  year={2022},
  publisher={Elsevier}
}
@inproceedings{takahashi2021differentiable,
  title={Differentiable fluids with solid coupling for learning and control},
  author={Takahashi, Tetsuya and Liang, Junbang and Qiao, Yi-Ling and Lin, Ming C},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={7},
  pages={6138--6146},
  year={2021}
}
@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}
@inproceedings{venkatraman2015improving,
  title={Improving multi-step prediction of learned time series models},
  author={Venkatraman, Arun and Hebert, Martial and Bagnell, J Andrew},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}
@article{jia2019neural,
  title={Neural jump stochastic differential equations},
  author={Jia, Junteng and Benson, Austin R},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{ashley2014sequential,
  title={A sequential Monte Carlo framework for the system identification of jump Markov state space models},
  author={Ashley, Trevor T and Andersson, Sean B},
  booktitle={2014 American Control Conference},
  pages={1144--1149},
  year={2014},
  organization={IEEE}
}
@article{opper2007variational,
  title={Variational inference for Markov jump processes},
  author={Opper, Manfred and Sanguinetti, Guido},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}
@article{fang2002stabilization,
  title={Stabilization of continuous-time jump linear systems},
  author={Fang, Yuguang and Loparo, Kenneth A},
  journal={IEEE Transactions on Automatic Control},
  volume={47},
  number={10},
  pages={1590--1603},
  year={2002},
  publisher={IEEE}
}
@inproceedings{svensson2014identification,
  title={Identification of jump Markov linear models using particle filters},
  author={Svensson, Andreas and Sch{\"o}n, Thomas B and Lindsten, Fredrik},
  booktitle={53rd IEEE conference on decision and control},
  pages={6504--6509},
  year={2014},
  organization={IEEE}
}
@article{ling2016reynolds,
  title={Reynolds averaged turbulence modelling using deep neural networks with embedded invariance},
  author={Ling, Julia and Kurzawski, Andrew and Templeton, Jeremy},
  journal={Journal of Fluid Mechanics},
  volume={807},
  pages={155--166},
  year={2016},
  publisher={Cambridge University Press}
}

@article{rackauckas2020universal,
  title={Universal differential equations for scientific machine learning},
  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
  journal={arXiv preprint arXiv:2001.04385},
  year={2020}
}
@article{ramadhan2020capturing,
  title={Capturing missing physics in climate model parameterizations using neural differential equations},
  author={Ramadhan, Ali and Marshall, John and Souza, Andre and Wagner, Gregory LeClaire and Ponnapati, Manvitha and Rackauckas, Christopher},
  journal={arXiv preprint arXiv:2010.12559},
  year={2020}
}
@book{powell2007approximate,
  title={Approximate Dynamic Programming: Solving the curses of dimensionality},
  author={Powell, Warren B},
  volume={703},
  year={2007},
  publisher={John Wiley \& Sons}
}
@book{zhang2012adaptive,
  title={Adaptive dynamic programming for control: algorithms and stability},
  author={Zhang, Huaguang and Liu, Derong and Luo, Yanhong and Wang, Ding},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@article{abu2005nearly,
  title={Nearly optimal control laws for nonlinear systems with saturating actuators using a neural network HJB approach},
  author={Abu-Khalaf, Murad and Lewis, Frank L},
  journal={Automatica},
  volume={41},
  number={5},
  pages={779--791},
  year={2005},
  publisher={Elsevier}
}
@article{vrabie2009neural,
  title={Neural network approach to continuous-time direct adaptive optimal control for partially unknown nonlinear systems},
  author={Vrabie, Draguna and Lewis, Frank},
  journal={Neural Networks},
  volume={22},
  number={3},
  pages={237--246},
  year={2009},
  publisher={Elsevier}
}
@article{vamvoudakis2014online,
  title={Online adaptive algorithm for optimal control with integral reinforcement learning},
  author={Vamvoudakis, Kyriakos G and Vrabie, Draguna and Lewis, Frank L},
  journal={International Journal of Robust and Nonlinear Control},
  volume={24},
  number={17},
  pages={2686--2710},
  year={2014},
  publisher={Wiley Online Library}
}
@article{zhang2008novel,
  title={A novel infinite-time optimal tracking control scheme for a class of discrete-time nonlinear systems via the greedy HDP iteration algorithm},
  author={Zhang, Huaguang and Wei, Qinglai and Luo, Yanhong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={38},
  number={4},
  pages={937--942},
  year={2008},
  publisher={IEEE}
}
@article{kamalapurkar2015approximate,
  title={Approximate optimal trajectory tracking for continuous-time nonlinear systems},
  author={Kamalapurkar, Rushikesh and Dinh, Huyen and Bhasin, Shubhendu and Dixon, Warren E},
  journal={Automatica},
  volume={51},
  pages={40--48},
  year={2015},
  publisher={Elsevier}
}
@article{modares2014linear,
  title={Linear quadratic tracking control of partially-unknown continuous-time systems using reinforcement learning},
  author={Modares, Hamidreza and Lewis, Frank L},
  journal={IEEE Transactions on Automatic control},
  volume={59},
  number={11},
  pages={3051--3056},
  year={2014},
  publisher={IEEE}
}
@article{modares2014optimal,
  title={Optimal tracking control of nonlinear partially-unknown constrained-input systems using integral reinforcement learning},
  author={Modares, Hamidreza and Lewis, Frank L},
  journal={Automatica},
  volume={50},
  number={7},
  pages={1780--1792},
  year={2014},
  publisher={Elsevier}
}
@article{jiang2012computational,
  title={Computational adaptive optimal control for continuous-time linear systems with completely unknown dynamics},
  author={Jiang, Yu and Jiang, Zhong-Ping},
  journal={Automatica},
  volume={48},
  number={10},
  pages={2699--2704},
  year={2012},
  publisher={Elsevier}
}
@article{modares2015h,
  title={H∞ Tracking control of completely unknown continuous-time systems via off-policy reinforcement learning},
  author={Modares, Hamidreza and Lewis, Frank L and Jiang, Zhong-Ping},
  journal={IEEE transactions on neural networks and learning systems},
  volume={26},
  number={10},
  pages={2550--2562},
  year={2015},
  publisher={IEEE}
}
@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@INPROCEEDINGS{7983780,
  author={Spielberg, S.P.K. and Gopaluni, R.B. and Loewen, P.D.},
  booktitle={2017 6th International Symposium on Advanced Control of Industrial Processes (AdCONIP)}, 
  title={Deep reinforcement learning approaches for process control}, 
  year={2017},
  volume={},
  number={},
  pages={201-206},
  doi={10.1109/ADCONIP.2017.7983780}}
@inproceedings{yu2017deep,
  title={Deep reinforcement learning based optimal trajectory tracking control of autonomous underwater vehicle},
  author={Yu, Runsheng and Shi, Zhenyu and Huang, Chaoxing and Li, Tenglong and Ma, Qiongxiong},
  booktitle={2017 36th Chinese control conference (CCC)},
  pages={4958--4965},
  year={2017},
  organization={IEEE}
}
@article{kim2018deep,
  title={Deep reinforcement learning based finite-horizon optimal tracking control for nonlinear system},
  author={Kim, Jong Woo and Park, Byung Jun and Yoo, Haeun and Lee, Jay H and Lee, Jong Min},
  journal={IFAC-PapersOnLine},
  volume={51},
  number={25},
  pages={257--262},
  year={2018},
  publisher={Elsevier}
}
@article{kim2020model,
  title={A model-based deep reinforcement learning method applied to finite-horizon optimal control of nonlinear control-affine system},
  author={Kim, Jong Woo and Park, Byung Jun and Yoo, Haeun and Oh, Tae Hoon and Lee, Jay H and Lee, Jong Min},
  journal={Journal of Process Control},
  volume={87},
  pages={166--178},
  year={2020},
  publisher={Elsevier}
}
@article{lecun2022path,
  title={A Path Towards Autonomous Machine Intelligence Version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  year={2022}
}
@article{moerland2020model,
  title={Model-based reinforcement learning: A survey},
  author={Moerland, Thomas M and Broekens, Joost and Jonker, Catholijn M},
  journal={arXiv preprint arXiv:2006.16712},
  year={2020}
}
@INPROCEEDINGS{6859280,
  author={Ashley, Trevor T. and Andersson, Sean B.},
  booktitle={2014 American Control Conference}, 
  title={A Sequential Monte Carlo framework for the system identification of jump Markov state space models}, 
  year={2014},
  volume={},
  number={},
  pages={1144-1149},
  doi={10.1109/ACC.2014.6859280}}

@article{Yilmaz2007,
author = {Yilmaz, Sezayi and Atik, Kemal},
doi = {10.1016/j.applthermaleng.2007.01.030},
issn = {13594311},
journal = {Applied Thermal Engineering},
keywords = {Artificial neural networks,Condenser temperature,Cooling cycle},
mendeley-groups = {cooling},
number = {13},
pages = {2308--2313},
title = {{Modeling of a mechanical cooling system with variable cooling capacity by using artificial neural network}},
volume = {27},
year = {2007}
}
@article{Watter2015,
author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {VAE},
pages = {2746--2754},
title = {{Embed to control: A locally linear latent dynamics model for control from raw images}},
volume = {2015-Janua},
year = {2015}
}
@article{song2015data,
  title={Data center energy and cost saving evaluation},
  author={Song, Zz and Zhang, Xiaojing and Eriksson, Clas},
  journal={Energy Procedia},
  volume={75},
  pages={1255--1260},
  year={2015},
  publisher={Elsevier}
}